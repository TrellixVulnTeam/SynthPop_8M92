{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\glede\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Iterable\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# For the Python notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations for stats and plots are based on: https://github.com/stasmix/popsynth/blob/master/pop-synth-vae.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_DATGAN(name):\n",
    "    if 'TGAN' in name or 'CTGAN' in name:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def compute_stats(freq_list_orig, freq_list_synth):\n",
    "    \"\"\"\n",
    "    Different statistics computed on the frequency list\n",
    "    \n",
    "    \"\"\"\n",
    "    freq_list_orig, freq_list_synth = np.array(freq_list_orig), np.array(freq_list_synth)\n",
    "    corr_mat = np.corrcoef(freq_list_orig, freq_list_synth)\n",
    "    corr = corr_mat[0, 1]\n",
    "    if np.isnan(corr): corr = 0.0\n",
    "    # MAE\n",
    "    mae = np.absolute(freq_list_orig - freq_list_synth).mean()\n",
    "    # RMSE\n",
    "    rmse = np.linalg.norm(freq_list_orig - freq_list_synth) / np.sqrt(len(freq_list_orig))\n",
    "    # SRMSE\n",
    "    freq_list_orig_avg = freq_list_orig.mean()\n",
    "    srmse = rmse / freq_list_orig_avg\n",
    "    # r-square\n",
    "    u = np.sum((freq_list_synth - freq_list_orig)**2)\n",
    "    v = np.sum((freq_list_orig - freq_list_orig_avg)**2)\n",
    "    r2 = 1.0 - u / v\n",
    "    stat = {'mae': mae, 'rmse': rmse, 'r2': r2, 'srmse': srmse, 'corr': corr}\n",
    "    \n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Chicago'\n",
    "\n",
    "input_folder = '../synth_data/{}/'.format(dataset)\n",
    "\n",
    "files_ = {}\n",
    "models = []\n",
    "\n",
    "for f in listdir(input_folder):\n",
    "    if isfile(join(input_folder, f)):\n",
    "        m = f.split('.')[0]\n",
    "        models.append(m)\n",
    "        files_[m] = join(input_folder, f)\n",
    "        \n",
    "models = ['WGAN_WI_01_NO_01', 'TEST']\n",
    "files_ = {}\n",
    "for m in models:\n",
    "    files_[m] = join(input_folder, m) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('../data/' + dataset + '/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset is 'Chicago':\n",
    "    continuous_cols = ['distance', 'age', 'departure_time']\n",
    "elif dataset is 'LPMC':\n",
    "    continuous_cols = ['start_time_linear', 'age', 'distance', 'dur_walking', 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', 'dur_pt_int', 'dur_driving', 'cost_transit', 'cost_driving_fuel', 'driving_traffic_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_cont = {}\n",
    "\n",
    "for c in continuous_cols:\n",
    "    #bins_cont[c] = pd.qcut(df_orig[c], q=10, retbins=True)[1]\n",
    "    bins_cont[c] = pd.cut(df_orig[c], bins=10, retbins=True)[1]\n",
    "    bins_cont[c][0] = -np.inf\n",
    "    bins_cont[c][-1] = np.inf\n",
    "    df_orig[c] = pd.cut(df_orig[c], bins=bins_cont[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choice</th>\n",
       "      <th>travel_dow</th>\n",
       "      <th>trip_purpose</th>\n",
       "      <th>distance</th>\n",
       "      <th>hh_vehicles</th>\n",
       "      <th>hh_size</th>\n",
       "      <th>hh_bikes</th>\n",
       "      <th>hh_descr</th>\n",
       "      <th>hh_income</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>license</th>\n",
       "      <th>education_level</th>\n",
       "      <th>work_status</th>\n",
       "      <th>departure_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drive</td>\n",
       "      <td>7</td>\n",
       "      <td>HOME_OTHER</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>detached</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>(29.4, 39.2]</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>PTE</td>\n",
       "      <td>(19.093, 21.48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>SHOPPING</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>detached</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>(49.0, 58.8]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>FTE</td>\n",
       "      <td>(16.707, 19.093]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>SHOPPING</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>detached</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>(78.4, 88.2]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>PTE</td>\n",
       "      <td>(7.16, 9.547]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>detached</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>(39.2, 49.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>FTE</td>\n",
       "      <td>(11.933, 14.32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>passenger</td>\n",
       "      <td>1</td>\n",
       "      <td>SHOPPING</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>detached</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>(29.4, 39.2]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>(9.547, 11.933]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      choice  travel_dow trip_purpose       distance  hh_vehicles  hh_size  \\\n",
       "0      drive           7   HOME_OTHER  (-inf, 6.971]            2        3   \n",
       "1      drive           2     SHOPPING  (-inf, 6.971]            3        3   \n",
       "2      drive           2     SHOPPING  (-inf, 6.971]            1        1   \n",
       "3      drive           2        OTHER  (-inf, 6.971]            2        2   \n",
       "4  passenger           1     SHOPPING  (-inf, 6.971]            2        2   \n",
       "\n",
       "   hh_bikes  hh_descr  hh_income  gender           age  license  \\\n",
       "0         3  detached          6       0  (29.4, 39.2]        1   \n",
       "1         3  detached          7       0  (49.0, 58.8]        1   \n",
       "2         0  detached          3       0  (78.4, 88.2]        1   \n",
       "3         0  detached          5       1  (39.2, 49.0]        1   \n",
       "4         1  detached          4       0  (29.4, 39.2]        0   \n",
       "\n",
       "   education_level work_status    departure_time  \n",
       "0                4         PTE   (19.093, 21.48]  \n",
       "1                5         FTE  (16.707, 19.093]  \n",
       "2                3         PTE     (7.16, 9.547]  \n",
       "3                5         FTE   (11.933, 14.32]  \n",
       "4                3  Unemployed   (9.547, 11.933]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_str = ['mae', 'rmse', 'r2', 'srmse', 'corr']\n",
    "orig_str = 'random-original'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per individual column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous results found, starting fresh\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/tests/stats/'\n",
    "filename = 'single_columns_{}.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stats for model \u001b[1mWGAN_WI_01_NO_01\u001b[0m (1/2)\n",
      "Preparing stats for model \u001b[1mTEST\u001b[0m (2/2)\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        df = pd.read_csv(files_[m])\n",
    "\n",
    "        # Discretize continuous columns\n",
    "        for c in continuous_cols:\n",
    "            df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in df_orig.columns:\n",
    "\n",
    "            agg_vars = [c]\n",
    "\n",
    "            real = df_orig.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = df.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            all_stats[m][c] = sts\n",
    "            \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_orig = {}\n",
    "\n",
    "train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "train.index = range(len(train))\n",
    "test = df_orig[~df_orig.index.isin(train.index)]\n",
    "test.index = range(len(test))\n",
    "\n",
    "# Go through each columns\n",
    "for c in df_orig.columns:\n",
    "\n",
    "    agg_vars = [c]\n",
    "\n",
    "    real = train.copy()\n",
    "    real['count'] = 1\n",
    "    real = real.groupby(agg_vars, observed=True).count()\n",
    "    real /= len(df_orig)\n",
    "\n",
    "    synth = test.copy()\n",
    "    synth['count'] = 1\n",
    "    synth = synth.groupby(agg_vars, observed=True).count()\n",
    "    synth /= len(df)\n",
    "\n",
    "    real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "    real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "    sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "    \n",
    "    stats_orig[c] = sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats[orig_str] = stats_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    res[test] = {}\n",
    "    \n",
    "    if test == 'all':\n",
    "        cols = df_orig.columns\n",
    "    elif test == 'cont':\n",
    "        cols = continuous_cols\n",
    "    elif test == 'cat':\n",
    "        cols = set(df_orig.columns) - set(continuous_cols)\n",
    "        \n",
    "    for s in stats_str:\n",
    "        res[test][s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "\n",
    "        for s in stats_str:\n",
    "            \n",
    "            tmp = []\n",
    "            for c in cols:\n",
    "                tmp.append(all_stats[m][c][s])\n",
    "            \n",
    "            res[test][s][m] = np.mean(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all columns based on SRMSE:\n",
      "   1. random-original      - 4.73e-02\n",
      "   2. TEST                 - 5.07e-02\n",
      "   3. WGAN_WI_01_NO_01     - 5.73e-02\n",
      "\n",
      "Ranking on continuous columns based on SRMSE:\n",
      "   1. random-original      - 3.75e-02\n",
      "   2. TEST                 - 1.03e-01\n",
      "   3. WGAN_WI_01_NO_01     - 1.38e-01\n",
      "\n",
      "Ranking on categorical columns based on SRMSE:\n",
      "   1. WGAN_WI_01_NO_01     - 3.72e-02\n",
      "   2. TEST                 - 3.76e-02\n",
      "   3. random-original      - 4.97e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    if test == 'all':\n",
    "        str_ = 'on all columns'\n",
    "    elif test == 'cont':\n",
    "        str_ = 'on continuous columns'\n",
    "    elif test == 'cat':\n",
    "        str_ = 'on categorical columns'\n",
    "        \n",
    "    for s in ['srmse']:#stats_str:\n",
    "        print('Ranking {} based on {}:'.format(str_, s.upper()))\n",
    "\n",
    "        if s in ['r2', 'corr']:\n",
    "            sorted_dct = {k: v for k, v in sorted(res[test][s].items(), key=lambda item: item[1])[::-1]}\n",
    "        else:\n",
    "            sorted_dct = {k: v for k, v in sorted(res[test][s].items(), key=lambda item: item[1])}\n",
    "\n",
    "        for i, item in enumerate(sorted_dct):\n",
    "            print('  {:>2}. {:<20} - {:.2e}'.format(i+1, item, sorted_dct[item]))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per couple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 105 combinations!\n"
     ]
    }
   ],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 2):\n",
    "    combs.append(k[0] + '::' + k[1])\n",
    "    \n",
    "print('There are {} combinations!'.format(len(combs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous results found, starting fresh\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/tests/stats/'\n",
    "filename = 'couple_columns_{}.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stats for model \u001b[1mWGAN_WI_01_NO_01\u001b[0m (1/2)\n",
      "Preparing stats for model \u001b[1mTEST\u001b[0m (2/2)\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "\n",
    "    else:\n",
    "    \n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        df = pd.read_csv(files_[m])\n",
    "\n",
    "        # Discretize continuous columns\n",
    "        for c in continuous_cols:\n",
    "            df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in combs:\n",
    "\n",
    "            agg_vars = c.split('::')\n",
    "\n",
    "            real = df_orig.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = df.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            all_stats[m][c] = sts\n",
    "            \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_orig = {}\n",
    "\n",
    "train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "train.index = range(len(train))\n",
    "test = df_orig[~df_orig.index.isin(train.index)]\n",
    "test.index = range(len(test))\n",
    "\n",
    "# Go through each columns\n",
    "for c in combs:\n",
    "\n",
    "    agg_vars = c.split('::')\n",
    "\n",
    "    real = train.copy()\n",
    "    real['count'] = 1\n",
    "    real = real.groupby(agg_vars, observed=True).count()\n",
    "    real /= len(df_orig)\n",
    "\n",
    "    synth = test.copy()\n",
    "    synth['count'] = 1\n",
    "    synth = synth.groupby(agg_vars, observed=True).count()\n",
    "    synth /= len(df)\n",
    "\n",
    "    real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "    real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "    sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "    \n",
    "    stats_orig[c] = sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats[orig_str] = stats_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "                    \n",
    "for m in all_stats.keys():\n",
    "\n",
    "    for s in stats_str:\n",
    "\n",
    "        tmp = []\n",
    "        for c in combs:\n",
    "            tmp.append(all_stats[m][c][s])\n",
    "\n",
    "        res[s][m] = np.mean(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all coupled combinations based on SRMSE:\n",
      "   1. random-original      - 1.29e-01\n",
      "   2. TEST                 - 1.49e-01\n",
      "   3. WGAN_WI_01_NO_01     - 1.80e-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats_str:\n",
    "    print('Ranking on all coupled combinations based on {}:'.format(s.upper()))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(res[s].items(), key=lambda item: item[1])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(res[s].items(), key=lambda item: item[1])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<20} - {:.2e}'.format(i+1, item, sorted_dct[item]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
