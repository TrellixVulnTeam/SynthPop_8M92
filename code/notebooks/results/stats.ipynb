{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\glede\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Iterable, defaultdict\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For the Python notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_DATGAN(name):\n",
    "    if any(x in name for x in ['TGAN', 'CTGAN', 'CTABGAN', 'TVAE', 'FULL', 'TRANSRED', 'LINEAR', 'NOLINKS', 'PREDICTION']):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def compute_stats(freq_list_orig, freq_list_synth):\n",
    "    \"\"\"\n",
    "    Different statistics computed on the frequency list\n",
    "    \n",
    "    \"\"\"\n",
    "    freq_list_orig, freq_list_synth = np.array(freq_list_orig), np.array(freq_list_synth)\n",
    "    corr_mat = np.corrcoef(freq_list_orig, freq_list_synth)\n",
    "    corr = corr_mat[0, 1]\n",
    "    if np.isnan(corr): corr = 0.0\n",
    "    # MAE\n",
    "    mae = np.absolute(freq_list_orig - freq_list_synth).mean()\n",
    "    # RMSE\n",
    "    rmse = np.linalg.norm(freq_list_orig - freq_list_synth) / np.sqrt(len(freq_list_orig))\n",
    "    # SRMSE\n",
    "    freq_list_orig_avg = freq_list_orig.mean()\n",
    "    srmse = rmse / freq_list_orig_avg\n",
    "    # r-square\n",
    "    u = np.sum((freq_list_synth - freq_list_orig)**2)\n",
    "    v = np.sum((freq_list_orig - freq_list_orig_avg)**2)\n",
    "    r2 = 1.0 - u / v\n",
    "    stat = {'mae': mae, 'rmse': rmse, 'r2': r2, 'srmse': srmse, 'corr': corr}\n",
    "    \n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all models and associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'adult'\n",
    "n_models = 5\n",
    "n_data = 5\n",
    "\n",
    "# Models for testing all DATGANS\n",
    "if 'adult' in dataset:\n",
    "    models = ['CTGAN', 'TGAN', 'TVAE', 'CTABGAN', 'WGGP_WI_NO', 'WGAN_WI_NO', 'LINEAR']\n",
    "else:\n",
    "    models = ['CTGAN', 'TGAN', 'TVAE', 'CTABGAN']\n",
    "\n",
    "    for i in ['WGAN', 'SGAN', 'WGGP']:\n",
    "        for j in ['WI', 'OR', 'WO']:\n",
    "            for k in ['NO', 'BO', 'OD', 'OC']:\n",
    "                models.append('{}_{}_{}'.format(i,j,k))\n",
    "            \n",
    "# Models for testing different DAGs\n",
    "if 'DAG' in dataset:\n",
    "    models = ['FULL', 'TRANSRED', 'LINEAR', 'NOLINKS', 'PREDICTION']\n",
    "            \n",
    "models.sort()\n",
    "\n",
    "files_ = {}\n",
    "\n",
    "for m in models:\n",
    "    tmp = []\n",
    "    if is_a_DATGAN(m):\n",
    "        spl = m.split('_')\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append('{}_{}_{:0>2}_{}_{:0>2}.csv'.format(spl[0], spl[1], i+1,  spl[2], j+1))\n",
    "    else:\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append('{}_{:0>2}_{:0>2}.csv'.format(m, i+1, j+1))\n",
    "    files_[m] = tmp\n",
    "\n",
    "\n",
    "input_folder = '../synth_data/{}/'.format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('../data/' + dataset.split('_')[0] + '/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Chicago' in dataset:\n",
    "    continuous_cols = ['distance', 'age', 'departure_time']\n",
    "elif 'LPMC' in dataset:\n",
    "    continuous_cols = ['start_time_linear', 'age', 'distance', 'dur_walking', 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', 'dur_pt_int', 'dur_driving', 'cost_transit', 'cost_driving_fuel', 'driving_traffic_percent']\n",
    "elif 'adult' in dataset:\n",
    "    continuous_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_cont = {}\n",
    "\n",
    "for c in continuous_cols:\n",
    "    #bins_cont[c] = pd.qcut(df_orig[c], q=10, retbins=True)[1]\n",
    "    bins_cont[c] = pd.cut(df_orig[c], bins=10, retbins=True)[1]\n",
    "    bins_cont[c][0] = -np.inf\n",
    "    bins_cont[c][-1] = np.inf\n",
    "    df_orig[c] = pd.cut(df_orig[c], bins=bins_cont[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_str = ['mae', 'rmse', 'r2', 'srmse', 'corr']\n",
    "orig_str = 'random-original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('./notebooks/results/{}'.format(dataset))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per individual column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous pickel file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'single_columns.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mCTABGAN\u001b[0m (1/7) already exists!\n",
      "Results for model \u001b[1mCTGAN\u001b[0m (2/7) already exists!\n",
      "Preparing stats for model \u001b[1mLINEAR\u001b[0m (3/7)\n",
      "Results for model \u001b[1mTGAN\u001b[0m (4/7) already exists!\n",
      "Results for model \u001b[1mTVAE\u001b[0m (5/7) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_NO\u001b[0m (6/7) already exists!\n",
      "Results for model \u001b[1mWGGP_WI_NO\u001b[0m (7/7) already exists!\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        for c in df_orig.columns:\n",
    "            all_stats[m][c] = {}\n",
    "            for s in stats_str:\n",
    "                all_stats[m][c][s] = []\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "\n",
    "        # Go through all dataframes generated for each model\n",
    "        for df in dfs:\n",
    "\n",
    "            # Discretize continuous columns\n",
    "            for c in continuous_cols:\n",
    "                df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "            # Go through each columns\n",
    "            for c in df_orig.columns:\n",
    "\n",
    "                agg_vars = [c]\n",
    "\n",
    "                real = df_orig.copy()\n",
    "                real['count'] = 1\n",
    "                real = real.groupby(agg_vars, observed=True).count()\n",
    "                real /= len(df_orig)\n",
    "\n",
    "                synth = df.copy()\n",
    "                synth['count'] = 1\n",
    "                synth = synth.groupby(agg_vars, observed=True).count()\n",
    "                synth /= len(df)\n",
    "\n",
    "                real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "                real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "                sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "                for s in sts:\n",
    "                    all_stats[m][c][s].append(sts[s])\n",
    "                    \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in all_stats:\n",
    "\n",
    "    stats_orig = {}\n",
    "\n",
    "    for c in df_orig.columns:\n",
    "        stats_orig[c] = {}\n",
    "        for s in stats_str:\n",
    "            stats_orig[c][s] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in df_orig.columns:\n",
    "\n",
    "            agg_vars = [c]\n",
    "\n",
    "            real = train.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = test.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                stats_orig[c][s].append(sts[s])\n",
    "    \n",
    "    all_stats[orig_str] = stats_orig\n",
    "    \n",
    "    pickle.dump(all_stats, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    res[test] = {}\n",
    "    \n",
    "    if test == 'all':\n",
    "        cols = df_orig.columns\n",
    "    elif test == 'cont':\n",
    "        cols = continuous_cols\n",
    "    elif test == 'cat':\n",
    "        cols = set(df_orig.columns) - set(continuous_cols)\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[test][s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "\n",
    "        for s in stats_str:\n",
    "            res[test][s][m] = []\n",
    "\n",
    "            for i in range(n_models*n_data):\n",
    "                tmp = []\n",
    "\n",
    "                for c in cols:\n",
    "                    tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "                res[test][s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    avg[test] = {}\n",
    "\n",
    "    for s in stats_str:\n",
    "        avg[test][s] = {}\n",
    "\n",
    "        for m in all_stats.keys():\n",
    "            avg[test][s][m] = {\n",
    "                'mean': np.mean(res[test][s][m]),\n",
    "                'std': np.std(res[test][s][m])\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all columns based on SRMSE:\n",
      "   1. random-original - 1.13e-02 ± 1.18e-03\n",
      "   2. LINEAR          - 4.01e-02 ± 4.84e-03\n",
      "   3. WGAN_WI_NO      - 4.12e-02 ± 1.79e-03\n",
      "   4. TGAN            - 7.48e-02 ± 8.47e-03\n",
      "   5. WGGP_WI_NO      - 1.11e-01 ± 5.42e-02\n",
      "   6. TVAE            - 1.18e-01 ± 2.40e-02\n",
      "   7. CTGAN           - 2.45e-01 ± 1.78e-02\n",
      "   8. CTABGAN         - 2.79e-01 ± 3.79e-02\n",
      "\n",
      "Ranking on continuous columns based on SRMSE:\n",
      "   1. random-original - 8.63e-03 ± 1.73e-03\n",
      "   2. LINEAR          - 6.60e-02 ± 8.34e-03\n",
      "   3. WGAN_WI_NO      - 7.13e-02 ± 9.32e-03\n",
      "   4. TGAN            - 9.62e-02 ± 1.28e-02\n",
      "   5. TVAE            - 1.04e-01 ± 2.42e-02\n",
      "   6. CTGAN           - 1.66e-01 ± 3.10e-02\n",
      "   7. WGGP_WI_NO      - 1.85e-01 ± 7.77e-02\n",
      "   8. CTABGAN         - 2.31e-01 ± 4.49e-02\n",
      "\n",
      "Ranking on categorical columns based on SRMSE:\n",
      "   1. random-original - 1.23e-02 ± 1.44e-03\n",
      "   2. WGAN_WI_NO      - 2.91e-02 ± 2.70e-03\n",
      "   3. LINEAR          - 2.97e-02 ± 6.40e-03\n",
      "   4. TGAN            - 6.62e-02 ± 1.08e-02\n",
      "   5. WGGP_WI_NO      - 8.13e-02 ± 4.69e-02\n",
      "   6. TVAE            - 1.24e-01 ± 2.77e-02\n",
      "   7. CTGAN           - 2.77e-01 ± 2.26e-02\n",
      "   8. CTABGAN         - 2.99e-01 ± 4.38e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    if test == 'all':\n",
    "        str_ = 'on all columns'\n",
    "    elif test == 'cont':\n",
    "        str_ = 'on continuous columns'\n",
    "    elif test == 'cat':\n",
    "        str_ = 'on categorical columns'\n",
    "        \n",
    "    for s in ['srmse']:#stats:\n",
    "        print('Ranking {} based on {}:'.format(str_, s.upper()))\n",
    "\n",
    "        if s in ['r2', 'corr']:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "        else:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "        for i, item in enumerate(sorted_dct):\n",
    "            print('  {:>2}. {:<15} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per couple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 2):\n",
    "    combs.append(k[0] + '::' + k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous pickel file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'couple_combinations.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mCTABGAN\u001b[0m (1/7) already exists!\n",
      "Results for model \u001b[1mCTGAN\u001b[0m (2/7) already exists!\n",
      "Preparing stats for model \u001b[1mLINEAR\u001b[0m (3/7)\n",
      "Results for model \u001b[1mTGAN\u001b[0m (4/7) already exists!\n",
      "Results for model \u001b[1mTVAE\u001b[0m (5/7) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_NO\u001b[0m (6/7) already exists!\n",
      "Results for model \u001b[1mWGGP_WI_NO\u001b[0m (7/7) already exists!\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        for c in combs:\n",
    "            all_stats[m][c] = {}\n",
    "            for s in stats_str:\n",
    "                all_stats[m][c][s] = []\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "\n",
    "        # Go through all dataframes generated for each model\n",
    "        for df in dfs:\n",
    "\n",
    "            # Discretize continuous columns\n",
    "            for c in continuous_cols:\n",
    "                df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "            # Go through each columns\n",
    "            for c in combs:\n",
    "\n",
    "                agg_vars = c.split('::')\n",
    "\n",
    "                real = df_orig.copy()\n",
    "                real['count'] = 1\n",
    "                real = real.groupby(agg_vars, observed=True).count()\n",
    "                real /= len(df_orig)\n",
    "\n",
    "                synth = df.copy()\n",
    "                synth['count'] = 1\n",
    "                synth = synth.groupby(agg_vars, observed=True).count()\n",
    "                synth /= len(df)\n",
    "\n",
    "                real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "                real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "                sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "                for s in sts:\n",
    "                    all_stats[m][c][s].append(sts[s])\n",
    "                    \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in all_stats:\n",
    "    stats_orig = {}\n",
    "\n",
    "    for c in combs:\n",
    "        stats_orig[c] = {}\n",
    "        for s in stats_str:\n",
    "            stats_orig[c][s] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in combs:\n",
    "\n",
    "            agg_vars = c.split('::')\n",
    "\n",
    "            real = train.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = test.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                stats_orig[c][s].append(sts[s])\n",
    "                \n",
    "    all_stats[orig_str] = stats_orig\n",
    "    \n",
    "    pickle.dump(all_stats, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in all_stats.keys():\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all coupled combinations based on SRMSE:\n",
      "   1. random-original - 4.35e-02 ± 3.22e-03\n",
      "   2. LINEAR          - 1.56e-01 ± 1.25e-02\n",
      "   3. WGAN_WI_NO      - 1.69e-01 ± 8.07e-03\n",
      "   4. TGAN            - 2.31e-01 ± 2.05e-02\n",
      "   5. WGGP_WI_NO      - 3.45e-01 ± 1.53e-01\n",
      "   6. TVAE            - 3.78e-01 ± 8.26e-02\n",
      "   7. CTGAN           - 7.44e-01 ± 5.53e-02\n",
      "   8. CTABGAN         - 9.00e-01 ± 9.30e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats:\n",
    "    print('Ranking on all coupled combinations based on {}:'.format(s.upper()))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<15} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per trouple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 3):\n",
    "    combs.append(k[0] + '::' + k[1] + '::' + k[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous pickel file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'trouple_combinations.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mCTABGAN\u001b[0m (1/7) already exists!\n",
      "Results for model \u001b[1mCTGAN\u001b[0m (2/7) already exists!\n",
      "Preparing stats for model \u001b[1mLINEAR\u001b[0m (3/7)\n",
      "Results for model \u001b[1mTGAN\u001b[0m (4/7) already exists!\n",
      "Results for model \u001b[1mTVAE\u001b[0m (5/7) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_NO\u001b[0m (6/7) already exists!\n",
      "Results for model \u001b[1mWGGP_WI_NO\u001b[0m (7/7) already exists!\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "\n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        for c in combs:\n",
    "            all_stats[m][c] = {}\n",
    "            for s in stats_str:\n",
    "                all_stats[m][c][s] = []\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "\n",
    "        # Go through all dataframes generated for each model\n",
    "        for df in dfs:\n",
    "\n",
    "            # Discretize continuous columns\n",
    "            for c in continuous_cols:\n",
    "                df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "            # Go through each columns\n",
    "            for c in combs:\n",
    "\n",
    "                agg_vars = c.split('::')\n",
    "\n",
    "                real = df_orig.copy()\n",
    "                real['count'] = 1\n",
    "                real = real.groupby(agg_vars, observed=True).count()\n",
    "                real /= len(df_orig)\n",
    "\n",
    "                synth = df.copy()\n",
    "                synth['count'] = 1\n",
    "                synth = synth.groupby(agg_vars, observed=True).count()\n",
    "                synth /= len(df)\n",
    "\n",
    "                real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "                real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "                sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "                for s in sts:\n",
    "                    all_stats[m][c][s].append(sts[s])\n",
    "    \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in all_stats:\n",
    "    stats_orig = {}\n",
    "\n",
    "    for c in combs:\n",
    "        stats_orig[c] = {}\n",
    "        for s in stats_str:\n",
    "            stats_orig[c][s] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in combs:\n",
    "\n",
    "            agg_vars = c.split('::')\n",
    "\n",
    "            real = train.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = test.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                stats_orig[c][s].append(sts[s])\n",
    "                \n",
    "    all_stats[orig_str] = stats_orig\n",
    "    \n",
    "    pickle.dump(all_stats, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in all_stats.keys():\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all triple combinations based on SRMSE:\n",
      "   1. random-original - 9.86e-02 ± 4.32e-03\n",
      "   2. LINEAR          - 3.76e-01 ± 2.40e-02\n",
      "   3. WGAN_WI_NO      - 4.18e-01 ± 1.79e-02\n",
      "   4. TGAN            - 4.55e-01 ± 3.26e-02\n",
      "   5. WGGP_WI_NO      - 7.11e-01 ± 2.89e-01\n",
      "   6. TVAE            - 7.70e-01 ± 1.59e-01\n",
      "   7. CTGAN           - 1.46e+00 ± 1.17e-01\n",
      "   8. CTABGAN         - 1.82e+00 ± 1.59e-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats_str:\n",
    "    print('Ranking on all triple combinations based on {}:'.format(s.upper()))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<15} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
