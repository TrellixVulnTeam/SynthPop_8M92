{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\glede\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Iterable, defaultdict\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For the Python notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_DATGAN(name):\n",
    "    if any(x in name for x in ['TGAN', 'CTGAN', 'FULL', 'TRANSRED', 'LINEAR', 'NOLINKS', 'PREDICTION']):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def compute_stats(freq_list_orig, freq_list_synth):\n",
    "    \"\"\"\n",
    "    Different statistics computed on the frequency list\n",
    "    \n",
    "    \"\"\"\n",
    "    freq_list_orig, freq_list_synth = np.array(freq_list_orig), np.array(freq_list_synth)\n",
    "    corr_mat = np.corrcoef(freq_list_orig, freq_list_synth)\n",
    "    corr = corr_mat[0, 1]\n",
    "    if np.isnan(corr): corr = 0.0\n",
    "    # MAE\n",
    "    mae = np.absolute(freq_list_orig - freq_list_synth).mean()\n",
    "    # RMSE\n",
    "    rmse = np.linalg.norm(freq_list_orig - freq_list_synth) / np.sqrt(len(freq_list_orig))\n",
    "    # SRMSE\n",
    "    freq_list_orig_avg = freq_list_orig.mean()\n",
    "    srmse = rmse / freq_list_orig_avg\n",
    "    # r-square\n",
    "    u = np.sum((freq_list_synth - freq_list_orig)**2)\n",
    "    v = np.sum((freq_list_orig - freq_list_orig_avg)**2)\n",
    "    r2 = 1.0 - u / v\n",
    "    stat = {'mae': mae, 'rmse': rmse, 'r2': r2, 'srmse': srmse, 'corr': corr}\n",
    "    \n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all models and associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Chicago_DAG'\n",
    "n_models = 5\n",
    "n_data = 5\n",
    "\n",
    "# Models for testing all DATGANS\n",
    "models = ['CTGAN', 'TGAN']\n",
    "\n",
    "for i in ['WGAN', 'SGAN', 'WGGP']:\n",
    "    for j in ['WI', 'OR', 'WO']:\n",
    "        for k in ['NO', 'BO', 'OD']:\n",
    "            models.append('{}_{}_{}'.format(i,j,k))\n",
    "            \n",
    "# Models for testing different DAGs\n",
    "models = ['FULL', 'TRANSRED', 'LINEAR', 'NOLINKS', 'PREDICTION']\n",
    "            \n",
    "models.sort()\n",
    "\n",
    "files_ = {}\n",
    "\n",
    "for m in models:\n",
    "    tmp = []\n",
    "    if is_a_DATGAN(m):\n",
    "        spl = m.split('_')\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append('{}_{}_{:0>2}_{}_{:0>2}.csv'.format(spl[0], spl[1], i+1,  spl[2], j+1))\n",
    "    else:\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append('{}_{:0>2}_{:0>2}.csv'.format(m, i+1, j+1))\n",
    "    files_[m] = tmp\n",
    "\n",
    "\n",
    "input_folder = '../synth_data/{}/'.format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('../data/' + dataset.split('_')[0] + '/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Chicago' in dataset:\n",
    "    continuous_cols = ['distance', 'age', 'departure_time']\n",
    "elif 'LPMC' in dataset:\n",
    "    continuous_cols = ['start_time_linear', 'age', 'distance', 'dur_walking', 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', 'dur_pt_int', 'dur_driving', 'cost_transit', 'cost_driving_fuel', 'driving_traffic_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_cont = {}\n",
    "\n",
    "for c in continuous_cols:\n",
    "    #bins_cont[c] = pd.qcut(df_orig[c], q=10, retbins=True)[1]\n",
    "    bins_cont[c] = pd.cut(df_orig[c], bins=10, retbins=True)[1]\n",
    "    bins_cont[c][0] = -np.inf\n",
    "    bins_cont[c][-1] = np.inf\n",
    "    df_orig[c] = pd.cut(df_orig[c], bins=bins_cont[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choice</th>\n",
       "      <th>travel_dow</th>\n",
       "      <th>trip_purpose</th>\n",
       "      <th>distance</th>\n",
       "      <th>hh_vehicles</th>\n",
       "      <th>hh_size</th>\n",
       "      <th>hh_bikes</th>\n",
       "      <th>hh_descr</th>\n",
       "      <th>hh_income</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>license</th>\n",
       "      <th>education_level</th>\n",
       "      <th>work_status</th>\n",
       "      <th>departure_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drive</td>\n",
       "      <td>7</td>\n",
       "      <td>HOME_OTHER</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>detached</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>(29.4, 39.2]</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>PTE</td>\n",
       "      <td>(19.093, 21.48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>SHOPPING</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>detached</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>(49.0, 58.8]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>FTE</td>\n",
       "      <td>(16.707, 19.093]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>SHOPPING</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>detached</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>(78.4, 88.2]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>PTE</td>\n",
       "      <td>(7.16, 9.547]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>detached</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>(39.2, 49.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>FTE</td>\n",
       "      <td>(11.933, 14.32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>passenger</td>\n",
       "      <td>1</td>\n",
       "      <td>SHOPPING</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>detached</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>(29.4, 39.2]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>(9.547, 11.933]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      choice  travel_dow trip_purpose       distance  hh_vehicles  hh_size  \\\n",
       "0      drive           7   HOME_OTHER  (-inf, 6.971]            2        3   \n",
       "1      drive           2     SHOPPING  (-inf, 6.971]            3        3   \n",
       "2      drive           2     SHOPPING  (-inf, 6.971]            1        1   \n",
       "3      drive           2        OTHER  (-inf, 6.971]            2        2   \n",
       "4  passenger           1     SHOPPING  (-inf, 6.971]            2        2   \n",
       "\n",
       "   hh_bikes  hh_descr  hh_income  gender           age  license  \\\n",
       "0         3  detached          6       0  (29.4, 39.2]        1   \n",
       "1         3  detached          7       0  (49.0, 58.8]        1   \n",
       "2         0  detached          3       0  (78.4, 88.2]        1   \n",
       "3         0  detached          5       1  (39.2, 49.0]        1   \n",
       "4         1  detached          4       0  (29.4, 39.2]        0   \n",
       "\n",
       "   education_level work_status    departure_time  \n",
       "0                4         PTE   (19.093, 21.48]  \n",
       "1                5         FTE  (16.707, 19.093]  \n",
       "2                3         PTE     (7.16, 9.547]  \n",
       "3                5         FTE   (11.933, 14.32]  \n",
       "4                3  Unemployed   (9.547, 11.933]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_str = ['mae', 'rmse', 'r2', 'srmse', 'corr']\n",
    "orig_str = 'random-original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('./notebooks/results/{}'.format(dataset))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per individual column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous pickel file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'single_columns.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mFULL\u001b[0m (1/5) already exists!\n",
      "Results for model \u001b[1mLINEAR\u001b[0m (2/5) already exists!\n",
      "Results for model \u001b[1mNOLINKS\u001b[0m (3/5) already exists!\n",
      "Results for model \u001b[1mPREDICTION\u001b[0m (4/5) already exists!\n",
      "Results for model \u001b[1mTRANSRED\u001b[0m (5/5) already exists!\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        for c in df_orig.columns:\n",
    "            all_stats[m][c] = {}\n",
    "            for s in stats_str:\n",
    "                all_stats[m][c][s] = []\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "\n",
    "        # Go through all dataframes generated for each model\n",
    "        for df in dfs:\n",
    "\n",
    "            # Discretize continuous columns\n",
    "            for c in continuous_cols:\n",
    "                df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "            # Go through each columns\n",
    "            for c in df_orig.columns:\n",
    "\n",
    "                agg_vars = [c]\n",
    "\n",
    "                real = df_orig.copy()\n",
    "                real['count'] = 1\n",
    "                real = real.groupby(agg_vars, observed=True).count()\n",
    "                real /= len(df_orig)\n",
    "\n",
    "                synth = df.copy()\n",
    "                synth['count'] = 1\n",
    "                synth = synth.groupby(agg_vars, observed=True).count()\n",
    "                synth /= len(df)\n",
    "\n",
    "                real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "                real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "                sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "                for s in sts:\n",
    "                    all_stats[m][c][s].append(sts[s])\n",
    "                    \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in all_stats:\n",
    "\n",
    "    stats_orig = {}\n",
    "\n",
    "    for c in df_orig.columns:\n",
    "        stats_orig[c] = {}\n",
    "        for s in stats_str:\n",
    "            stats_orig[c][s] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in df_orig.columns:\n",
    "\n",
    "            agg_vars = [c]\n",
    "\n",
    "            real = train.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = test.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                stats_orig[c][s].append(sts[s])\n",
    "    \n",
    "    all_stats[orig_str] = stats_orig\n",
    "    \n",
    "    pickle.dump(all_stats, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    res[test] = {}\n",
    "    \n",
    "    if test == 'all':\n",
    "        cols = df_orig.columns\n",
    "    elif test == 'cont':\n",
    "        cols = continuous_cols\n",
    "    elif test == 'cat':\n",
    "        cols = set(df_orig.columns) - set(continuous_cols)\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[test][s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "\n",
    "        for s in stats_str:\n",
    "            res[test][s][m] = []\n",
    "\n",
    "            for i in range(n_models*n_data):\n",
    "                tmp = []\n",
    "\n",
    "                for c in cols:\n",
    "                    tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "                res[test][s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    avg[test] = {}\n",
    "\n",
    "    for s in stats_str:\n",
    "        avg[test][s] = {}\n",
    "\n",
    "        for m in all_stats.keys():\n",
    "            avg[test][s][m] = {\n",
    "                'mean': np.mean(res[test][s][m]),\n",
    "                'std': np.std(res[test][s][m])\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all columns based on SRMSE:\n",
      "   1. random-original      - 5.25e-02 ± 2.85e-03\n",
      "   2. NOLINKS              - 5.90e-02 ± 5.88e-03\n",
      "   3. LINEAR               - 6.00e-02 ± 4.29e-03\n",
      "   4. TRANSRED             - 6.19e-02 ± 6.07e-03\n",
      "   5. PREDICTION           - 6.25e-02 ± 2.72e-03\n",
      "   6. FULL                 - 6.49e-02 ± 9.76e-03\n",
      "\n",
      "Ranking on continuous columns based on SRMSE:\n",
      "   1. random-original      - 4.29e-02 ± 3.76e-03\n",
      "   2. FULL                 - 1.26e-01 ± 2.11e-02\n",
      "   3. LINEAR               - 1.32e-01 ± 1.94e-02\n",
      "   4. TRANSRED             - 1.35e-01 ± 9.41e-03\n",
      "   5. NOLINKS              - 1.77e-01 ± 3.07e-02\n",
      "   6. PREDICTION           - 1.96e-01 ± 1.20e-02\n",
      "\n",
      "Ranking on categorical columns based on SRMSE:\n",
      "   1. PREDICTION           - 2.91e-02 ± 2.49e-03\n",
      "   2. NOLINKS              - 2.94e-02 ± 3.04e-03\n",
      "   3. LINEAR               - 4.20e-02 ± 3.56e-03\n",
      "   4. TRANSRED             - 4.36e-02 ± 5.83e-03\n",
      "   5. FULL                 - 4.95e-02 ± 7.94e-03\n",
      "   6. random-original      - 5.49e-02 ± 3.55e-03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    if test == 'all':\n",
    "        str_ = 'on all columns'\n",
    "    elif test == 'cont':\n",
    "        str_ = 'on continuous columns'\n",
    "    elif test == 'cat':\n",
    "        str_ = 'on categorical columns'\n",
    "        \n",
    "    for s in ['srmse']:#stats:\n",
    "        print('Ranking {} based on {}:'.format(str_, s.upper()))\n",
    "\n",
    "        if s in ['r2', 'corr']:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "        else:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "        for i, item in enumerate(sorted_dct):\n",
    "            print('  {:>2}. {:<20} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per couple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 2):\n",
    "    combs.append(k[0] + '::' + k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous pickel file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'couple_combinations.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mFULL\u001b[0m (1/5) already exists!\n",
      "Results for model \u001b[1mLINEAR\u001b[0m (2/5) already exists!\n",
      "Results for model \u001b[1mNOLINKS\u001b[0m (3/5) already exists!\n",
      "Results for model \u001b[1mPREDICTION\u001b[0m (4/5) already exists!\n",
      "Results for model \u001b[1mTRANSRED\u001b[0m (5/5) already exists!\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        for c in combs:\n",
    "            all_stats[m][c] = {}\n",
    "            for s in stats_str:\n",
    "                all_stats[m][c][s] = []\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "\n",
    "        # Go through all dataframes generated for each model\n",
    "        for df in dfs:\n",
    "\n",
    "            # Discretize continuous columns\n",
    "            for c in continuous_cols:\n",
    "                df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "            # Go through each columns\n",
    "            for c in combs:\n",
    "\n",
    "                agg_vars = c.split('::')\n",
    "\n",
    "                real = df_orig.copy()\n",
    "                real['count'] = 1\n",
    "                real = real.groupby(agg_vars, observed=True).count()\n",
    "                real /= len(df_orig)\n",
    "\n",
    "                synth = df.copy()\n",
    "                synth['count'] = 1\n",
    "                synth = synth.groupby(agg_vars, observed=True).count()\n",
    "                synth /= len(df)\n",
    "\n",
    "                real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "                real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "                sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "                for s in sts:\n",
    "                    all_stats[m][c][s].append(sts[s])\n",
    "                    \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in all_stats:\n",
    "    stats_orig = {}\n",
    "\n",
    "    for c in combs:\n",
    "        stats_orig[c] = {}\n",
    "        for s in stats_str:\n",
    "            stats_orig[c][s] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in combs:\n",
    "\n",
    "            agg_vars = c.split('::')\n",
    "\n",
    "            real = train.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = test.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                stats_orig[c][s].append(sts[s])\n",
    "                \n",
    "    all_stats[orig_str] = stats_orig\n",
    "    \n",
    "    pickle.dump(all_stats, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in all_stats.keys():\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all coupled combinations based on SRMSE:\n",
      "   1. random-original      - 1.28e-01 ± 4.67e-03\n",
      "   2. TRANSRED             - 1.81e-01 ± 1.04e-02\n",
      "   3. LINEAR               - 1.84e-01 ± 9.46e-03\n",
      "   4. FULL                 - 1.85e-01 ± 1.40e-02\n",
      "   5. NOLINKS              - 3.48e-01 ± 1.14e-02\n",
      "   6. PREDICTION           - 3.48e-01 ± 4.71e-03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats:\n",
    "    print('Ranking on all coupled combinations based on {}:'.format(s.upper()))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<20} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per trouple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 3):\n",
    "    combs.append(k[0] + '::' + k[1] + '::' + k[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous pickel file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'trouple_combinations.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mFULL\u001b[0m (1/5) already exists!\n",
      "Results for model \u001b[1mLINEAR\u001b[0m (2/5) already exists!\n",
      "Results for model \u001b[1mNOLINKS\u001b[0m (3/5) already exists!\n",
      "Results for model \u001b[1mPREDICTION\u001b[0m (4/5) already exists!\n",
      "Results for model \u001b[1mTRANSRED\u001b[0m (5/5) already exists!\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "\n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        for c in combs:\n",
    "            all_stats[m][c] = {}\n",
    "            for s in stats_str:\n",
    "                all_stats[m][c][s] = []\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "\n",
    "        # Go through all dataframes generated for each model\n",
    "        for df in dfs:\n",
    "\n",
    "            # Discretize continuous columns\n",
    "            for c in continuous_cols:\n",
    "                df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "            # Go through each columns\n",
    "            for c in combs:\n",
    "\n",
    "                agg_vars = c.split('::')\n",
    "\n",
    "                real = df_orig.copy()\n",
    "                real['count'] = 1\n",
    "                real = real.groupby(agg_vars, observed=True).count()\n",
    "                real /= len(df_orig)\n",
    "\n",
    "                synth = df.copy()\n",
    "                synth['count'] = 1\n",
    "                synth = synth.groupby(agg_vars, observed=True).count()\n",
    "                synth /= len(df)\n",
    "\n",
    "                real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "                real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "                sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "                for s in sts:\n",
    "                    all_stats[m][c][s].append(sts[s])\n",
    "    \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in all_stats:\n",
    "    stats_orig = {}\n",
    "\n",
    "    for c in combs:\n",
    "        stats_orig[c] = {}\n",
    "        for s in stats_str:\n",
    "            stats_orig[c][s] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in combs:\n",
    "\n",
    "            agg_vars = c.split('::')\n",
    "\n",
    "            real = train.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = test.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                stats_orig[c][s].append(sts[s])\n",
    "                \n",
    "    all_stats[orig_str] = stats_orig\n",
    "    \n",
    "    pickle.dump(all_stats, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in all_stats.keys():\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all triple combinations based on SRMSE:\n",
      "   1. random-original      - 2.40e-01 ± 9.08e-03\n",
      "   2. TRANSRED             - 3.76e-01 ± 1.45e-02\n",
      "   3. FULL                 - 3.80e-01 ± 1.84e-02\n",
      "   4. LINEAR               - 3.90e-01 ± 1.56e-02\n",
      "   5. PREDICTION           - 7.85e-01 ± 8.43e-03\n",
      "   6. NOLINKS              - 7.91e-01 ± 1.42e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats_str:\n",
    "    print('Ranking on all triple combinations based on {}:'.format(s.upper()))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<20} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
