{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Iterable, defaultdict\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For the Python notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_DATGAN(name):\n",
    "    if 'TGAN' in name or 'CTGAN' in name:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def compute_stats(freq_list_orig, freq_list_synth):\n",
    "    \"\"\"\n",
    "    Different statistics computed on the frequency list\n",
    "    \n",
    "    \"\"\"\n",
    "    freq_list_orig, freq_list_synth = np.array(freq_list_orig), np.array(freq_list_synth)\n",
    "    corr_mat = np.corrcoef(freq_list_orig, freq_list_synth)\n",
    "    corr = corr_mat[0, 1]\n",
    "    if np.isnan(corr): corr = 0.0\n",
    "    # MAE\n",
    "    mae = np.absolute(freq_list_orig - freq_list_synth).mean()\n",
    "    # RMSE\n",
    "    rmse = np.linalg.norm(freq_list_orig - freq_list_synth) / np.sqrt(len(freq_list_orig))\n",
    "    # SRMSE\n",
    "    freq_list_orig_avg = freq_list_orig.mean()\n",
    "    srmse = rmse / freq_list_orig_avg\n",
    "    # r-square\n",
    "    u = np.sum((freq_list_synth - freq_list_orig)**2)\n",
    "    v = np.sum((freq_list_orig - freq_list_orig_avg)**2)\n",
    "    r2 = 1.0 - u / v\n",
    "    stat = {'mae': mae, 'rmse': rmse, 'r2': r2, 'srmse': srmse, 'corr': corr}\n",
    "    \n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all models and associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Chicago'\n",
    "n_models = 5\n",
    "n_data = 5\n",
    "\n",
    "models = ['CTGAN', 'TGAN']\n",
    "\n",
    "for i in ['WGAN', 'SGAN', 'WGGP']:\n",
    "    for j in ['WI', 'OR', 'WO']:\n",
    "        for k in ['NO', 'BO', 'OD']:\n",
    "            models.append('{}_{}_{}'.format(i,j,k))\n",
    "            \n",
    "models.sort()\n",
    "\n",
    "files_ = {}\n",
    "\n",
    "for m in models:\n",
    "    tmp = []\n",
    "    if is_a_DATGAN(m):\n",
    "        spl = m.split('_')\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append('{}_{}_{:0>2}_{}_{:0>2}.csv'.format(spl[0], spl[1], i+1,  spl[2], j+1))\n",
    "    else:\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append('{}_{:0>2}_{:0>2}.csv'.format(m, i+1, j+1))\n",
    "    files_[m] = tmp\n",
    "\n",
    "\n",
    "input_folder = '../synth_data/{}/'.format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('../data/' + dataset + '/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset is 'Chicago':\n",
    "    continuous_cols = ['distance', 'age', 'departure_time']\n",
    "elif dataset is 'LPMC':\n",
    "    continuous_cols = ['start_time_linear', 'age', 'distance', 'dur_walking', 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', 'dur_pt_int', 'dur_driving', 'cost_transit', 'cost_driving_fuel', 'driving_traffic_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_cont = {}\n",
    "\n",
    "for c in continuous_cols:\n",
    "    #bins_cont[c] = pd.qcut(df_orig[c], q=10, retbins=True)[1]\n",
    "    bins_cont[c] = pd.cut(df_orig[c], bins=10, retbins=True)[1]\n",
    "    bins_cont[c][0] = -np.inf\n",
    "    bins_cont[c][-1] = np.inf\n",
    "    df_orig[c] = pd.cut(df_orig[c], bins=bins_cont[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choice</th>\n",
       "      <th>travel_dow</th>\n",
       "      <th>trip_purpose</th>\n",
       "      <th>distance</th>\n",
       "      <th>hh_vehicles</th>\n",
       "      <th>hh_size</th>\n",
       "      <th>hh_bikes</th>\n",
       "      <th>hh_descr</th>\n",
       "      <th>hh_income</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>license</th>\n",
       "      <th>education_level</th>\n",
       "      <th>work_status</th>\n",
       "      <th>departure_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drive</td>\n",
       "      <td>7</td>\n",
       "      <td>HOME_OTHER</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>detached</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>(29.4, 39.2]</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>PTE</td>\n",
       "      <td>(19.093, 21.48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>SHOPPING</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>detached</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>(49.0, 58.8]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>FTE</td>\n",
       "      <td>(16.707, 19.093]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>SHOPPING</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>detached</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>(78.4, 88.2]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>PTE</td>\n",
       "      <td>(7.16, 9.547]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drive</td>\n",
       "      <td>2</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>detached</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>(39.2, 49.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>FTE</td>\n",
       "      <td>(11.933, 14.32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>passenger</td>\n",
       "      <td>1</td>\n",
       "      <td>SHOPPING</td>\n",
       "      <td>(-inf, 6.971]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>detached</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>(29.4, 39.2]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>(9.547, 11.933]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      choice  travel_dow trip_purpose       distance  hh_vehicles  hh_size  \\\n",
       "0      drive           7   HOME_OTHER  (-inf, 6.971]            2        3   \n",
       "1      drive           2     SHOPPING  (-inf, 6.971]            3        3   \n",
       "2      drive           2     SHOPPING  (-inf, 6.971]            1        1   \n",
       "3      drive           2        OTHER  (-inf, 6.971]            2        2   \n",
       "4  passenger           1     SHOPPING  (-inf, 6.971]            2        2   \n",
       "\n",
       "   hh_bikes  hh_descr  hh_income  gender           age  license  \\\n",
       "0         3  detached          6       0  (29.4, 39.2]        1   \n",
       "1         3  detached          7       0  (49.0, 58.8]        1   \n",
       "2         0  detached          3       0  (78.4, 88.2]        1   \n",
       "3         0  detached          5       1  (39.2, 49.0]        1   \n",
       "4         1  detached          4       0  (29.4, 39.2]        0   \n",
       "\n",
       "   education_level work_status    departure_time  \n",
       "0                4         PTE   (19.093, 21.48]  \n",
       "1                5         FTE  (16.707, 19.093]  \n",
       "2                3         PTE     (7.16, 9.547]  \n",
       "3                5         FTE   (11.933, 14.32]  \n",
       "4                3  Unemployed   (9.547, 11.933]  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_str = ['mae', 'rmse', 'r2', 'srmse', 'corr']\n",
    "orig_str = 'random-original'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per individual column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stats for model \u001b[1mCTGAN\u001b[0m (1/29)\n",
      "Preparing stats for model \u001b[1mSGAN_OR_BO\u001b[0m (2/29)\n",
      "Preparing stats for model \u001b[1mSGAN_OR_NO\u001b[0m (3/29)\n",
      "Preparing stats for model \u001b[1mSGAN_OR_OD\u001b[0m (4/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WI_BO\u001b[0m (5/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WI_NO\u001b[0m (6/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WI_OD\u001b[0m (7/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WO_BO\u001b[0m (8/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WO_NO\u001b[0m (9/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WO_OD\u001b[0m (10/29)\n",
      "Preparing stats for model \u001b[1mTGAN\u001b[0m (11/29)\n",
      "Preparing stats for model \u001b[1mWGAN_OR_BO\u001b[0m (12/29)\n",
      "Preparing stats for model \u001b[1mWGAN_OR_NO\u001b[0m (13/29)\n",
      "Preparing stats for model \u001b[1mWGAN_OR_OD\u001b[0m (14/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WI_BO\u001b[0m (15/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WI_NO\u001b[0m (16/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WI_OD\u001b[0m (17/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WO_BO\u001b[0m (18/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WO_NO\u001b[0m (19/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WO_OD\u001b[0m (20/29)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_BO\u001b[0m (21/29)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_NO\u001b[0m (22/29)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_OD\u001b[0m (23/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_BO\u001b[0m (24/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_NO\u001b[0m (25/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_OD\u001b[0m (26/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_BO\u001b[0m (27/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_NO\u001b[0m (28/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_OD\u001b[0m (29/29)\n"
     ]
    }
   ],
   "source": [
    "all_stats = {}\n",
    "\n",
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "    \n",
    "    all_stats[m] = {}\n",
    "    \n",
    "    for c in df_orig.columns:\n",
    "        all_stats[m][c] = {}\n",
    "        for s in stats_str:\n",
    "            all_stats[m][c][s] = []\n",
    "    \n",
    "    # Load all dataframes for current model\n",
    "    dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "    \n",
    "    # Go through all dataframes generated for each model\n",
    "    for df in dfs:\n",
    "        \n",
    "        # Discretize continuous columns\n",
    "        for c in continuous_cols:\n",
    "            df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "        \n",
    "        # Go through each columns\n",
    "        for c in df_orig.columns:\n",
    "            \n",
    "            agg_vars = [c]\n",
    "            \n",
    "            real = df_orig.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "            \n",
    "            synth = df.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "            \n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "            \n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "            \n",
    "            for s in sts:\n",
    "                all_stats[m][c][s].append(sts[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_orig = {}\n",
    "\n",
    "for c in df_orig.columns:\n",
    "    stats_orig[c] = {}\n",
    "    for s in stats_str:\n",
    "        stats_orig[c][s] = []\n",
    "\n",
    "for i in range(n_models*n_data):\n",
    "\n",
    "    train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "    train.index = range(len(train))\n",
    "    test = df_orig[~df_orig.index.isin(train.index)]\n",
    "    test.index = range(len(test))\n",
    "\n",
    "    # Go through each columns\n",
    "    for c in df_orig.columns:\n",
    "\n",
    "        agg_vars = [c]\n",
    "\n",
    "        real = train.copy()\n",
    "        real['count'] = 1\n",
    "        real = real.groupby(agg_vars, observed=True).count()\n",
    "        real /= len(df_orig)\n",
    "\n",
    "        synth = test.copy()\n",
    "        synth['count'] = 1\n",
    "        synth = synth.groupby(agg_vars, observed=True).count()\n",
    "        synth /= len(df)\n",
    "\n",
    "        real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "        real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "        sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "        for s in sts:\n",
    "            stats_orig[c][s].append(sts[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats[orig_str] = stats_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./notebooks/results/single_columns.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(all_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./notebooks/results/single_columns.json', 'r') as infile:\n",
    "    all_stats = json.loads(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    res[test] = {}\n",
    "    \n",
    "    if test == 'all':\n",
    "        cols = df_orig.columns\n",
    "    elif test == 'cont':\n",
    "        cols = continuous_cols\n",
    "    elif test == 'cat':\n",
    "        cols = set(df_orig.columns) - set(continuous_cols)\n",
    "\n",
    "    for s in stats:\n",
    "        res[test][s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "\n",
    "        for s in stats:\n",
    "            res[test][s][m] = []\n",
    "\n",
    "            for i in range(n_models*n_data):\n",
    "                tmp = []\n",
    "\n",
    "                for c in cols:\n",
    "                    tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "                res[test][s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    avg[test] = {}\n",
    "\n",
    "    for s in stats:\n",
    "        avg[test][s] = {}\n",
    "\n",
    "        for m in all_stats.keys():\n",
    "            avg[test][s][m] = {\n",
    "                'mean': np.mean(res[test][s][m]),\n",
    "                'std': np.std(res[test][s][m])\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all columns based on SRMSE:\n",
      "   1. random-original      - 5.38e-02 ± 3.13e-03\n",
      "   2. WGAN_WI_NO           - 6.31e-02 ± 6.91e-03\n",
      "   3. WGAN_WO_NO           - 6.71e-02 ± 8.64e-03\n",
      "   4. SGAN_WO_NO           - 6.72e-02 ± 5.85e-03\n",
      "   5. SGAN_WI_NO           - 7.09e-02 ± 1.08e-02\n",
      "   6. SGAN_WO_OD           - 7.15e-02 ± 6.20e-03\n",
      "   7. SGAN_WI_OD           - 7.25e-02 ± 1.02e-02\n",
      "   8. SGAN_WO_BO           - 7.37e-02 ± 5.87e-03\n",
      "   9. WGAN_OR_BO           - 7.54e-02 ± 7.72e-03\n",
      "  10. SGAN_WI_BO           - 7.58e-02 ± 1.11e-02\n",
      "  11. WGAN_OR_OD           - 7.60e-02 ± 6.86e-03\n",
      "  12. WGAN_WI_BO           - 7.68e-02 ± 8.90e-03\n",
      "  13. SGAN_OR_OD           - 7.80e-02 ± 6.31e-03\n",
      "  14. WGAN_WI_OD           - 7.98e-02 ± 1.07e-02\n",
      "  15. SGAN_OR_BO           - 8.11e-02 ± 5.76e-03\n",
      "  16. WGGP_WO_NO           - 9.31e-02 ± 1.86e-02\n",
      "  17. WGAN_WO_OD           - 1.00e-01 ± 1.13e-02\n",
      "  18. WGAN_WO_BO           - 1.02e-01 ± 1.47e-02\n",
      "  19. WGGP_WO_OD           - 1.03e-01 ± 2.05e-02\n",
      "  20. WGGP_WO_BO           - 1.08e-01 ± 2.53e-02\n",
      "  21. TGAN                 - 1.14e-01 ± 2.15e-02\n",
      "  22. WGGP_WI_NO           - 1.19e-01 ± 3.61e-02\n",
      "  23. WGGP_WI_OD           - 1.34e-01 ± 4.63e-02\n",
      "  24. WGGP_WI_BO           - 1.45e-01 ± 5.76e-02\n",
      "  25. WGGP_OR_OD           - 1.80e-01 ± 5.20e-02\n",
      "  26. WGGP_OR_BO           - 1.83e-01 ± 5.31e-02\n",
      "  27. CTGAN                - 2.60e-01 ± 2.01e-02\n",
      "  28. SGAN_OR_NO           - 2.64e-01 ± 4.74e-03\n",
      "  29. WGAN_OR_NO           - 2.65e-01 ± 6.03e-03\n",
      "  30. WGGP_OR_NO           - 3.08e-01 ± 3.72e-02\n",
      "\n",
      "Ranking on continuous columns based on SRMSE:\n",
      "   1. random-original      - 4.52e-02 ± 5.25e-03\n",
      "   2. WGAN_WI_BO           - 1.06e-01 ± 2.94e-02\n",
      "   3. WGAN_WI_NO           - 1.25e-01 ± 1.12e-02\n",
      "   4. SGAN_OR_OD           - 1.26e-01 ± 1.90e-02\n",
      "   5. WGAN_WI_OD           - 1.26e-01 ± 1.12e-02\n",
      "   6. SGAN_OR_NO           - 1.27e-01 ± 2.02e-02\n",
      "   7. WGAN_OR_NO           - 1.29e-01 ± 1.88e-02\n",
      "   8. WGAN_OR_OD           - 1.31e-01 ± 2.05e-02\n",
      "   9. WGAN_OR_BO           - 1.32e-01 ± 2.08e-02\n",
      "  10. SGAN_OR_BO           - 1.41e-01 ± 2.00e-02\n",
      "  11. SGAN_WO_OD           - 1.41e-01 ± 2.70e-02\n",
      "  12. SGAN_WO_NO           - 1.41e-01 ± 2.69e-02\n",
      "  13. SGAN_WI_NO           - 1.42e-01 ± 2.56e-02\n",
      "  14. SGAN_WI_OD           - 1.44e-01 ± 2.82e-02\n",
      "  15. SGAN_WO_BO           - 1.50e-01 ± 2.76e-02\n",
      "  16. WGAN_WO_OD           - 1.56e-01 ± 2.46e-02\n",
      "  17. WGAN_WO_NO           - 1.57e-01 ± 2.79e-02\n",
      "  18. WGAN_WO_BO           - 1.64e-01 ± 4.66e-02\n",
      "  19. SGAN_WI_BO           - 1.66e-01 ± 3.08e-02\n",
      "  20. WGGP_WO_OD           - 2.61e-01 ± 9.19e-02\n",
      "  21. WGGP_WO_NO           - 2.65e-01 ± 9.50e-02\n",
      "  22. WGGP_WO_BO           - 2.82e-01 ± 1.23e-01\n",
      "  23. WGGP_WI_OD           - 2.85e-01 ± 1.17e-01\n",
      "  24. WGGP_WI_NO           - 2.88e-01 ± 1.15e-01\n",
      "  25. CTGAN                - 2.90e-01 ± 6.09e-02\n",
      "  26. TGAN                 - 3.05e-01 ± 7.76e-02\n",
      "  27. WGGP_OR_OD           - 3.28e-01 ± 1.69e-01\n",
      "  28. WGGP_OR_NO           - 3.31e-01 ± 1.68e-01\n",
      "  29. WGGP_WI_BO           - 3.37e-01 ± 1.67e-01\n",
      "  30. WGGP_OR_BO           - 3.39e-01 ± 1.78e-01\n",
      "\n",
      "Ranking on categorical columns based on SRMSE:\n",
      "   1. WGAN_WO_NO           - 4.46e-02 ± 8.71e-03\n",
      "   2. WGAN_WI_NO           - 4.77e-02 ± 8.45e-03\n",
      "   3. SGAN_WO_NO           - 4.89e-02 ± 5.67e-03\n",
      "   4. WGGP_WO_NO           - 5.01e-02 ± 1.14e-02\n",
      "   5. SGAN_WI_BO           - 5.32e-02 ± 7.02e-03\n",
      "   6. SGAN_WI_NO           - 5.32e-02 ± 8.60e-03\n",
      "   7. SGAN_WO_OD           - 5.42e-02 ± 6.26e-03\n",
      "   8. SGAN_WO_BO           - 5.46e-02 ± 5.42e-03\n",
      "   9. SGAN_WI_OD           - 5.47e-02 ± 7.29e-03\n",
      "  10. random-original      - 5.60e-02 ± 4.10e-03\n",
      "  11. WGAN_OR_BO           - 6.12e-02 ± 7.22e-03\n",
      "  12. WGAN_OR_OD           - 6.21e-02 ± 7.88e-03\n",
      "  13. WGGP_WO_OD           - 6.37e-02 ± 1.28e-02\n",
      "  14. WGGP_WO_BO           - 6.39e-02 ± 1.29e-02\n",
      "  15. SGAN_OR_OD           - 6.60e-02 ± 6.83e-03\n",
      "  16. SGAN_OR_BO           - 6.62e-02 ± 5.59e-03\n",
      "  17. TGAN                 - 6.68e-02 ± 1.13e-02\n",
      "  18. WGAN_WI_OD           - 6.83e-02 ± 1.42e-02\n",
      "  19. WGAN_WI_BO           - 6.94e-02 ± 1.28e-02\n",
      "  20. WGGP_WI_NO           - 7.71e-02 ± 1.70e-02\n",
      "  21. WGAN_WO_BO           - 8.63e-02 ± 8.41e-03\n",
      "  22. WGAN_WO_OD           - 8.66e-02 ± 8.77e-03\n",
      "  23. WGGP_WI_OD           - 9.68e-02 ± 3.81e-02\n",
      "  24. WGGP_WI_BO           - 9.68e-02 ± 3.89e-02\n",
      "  25. WGGP_OR_BO           - 1.43e-01 ± 2.68e-02\n",
      "  26. WGGP_OR_OD           - 1.44e-01 ± 2.75e-02\n",
      "  27. CTGAN                - 2.53e-01 ± 2.02e-02\n",
      "  28. SGAN_OR_NO           - 2.98e-01 ± 6.03e-03\n",
      "  29. WGAN_OR_NO           - 2.99e-01 ± 6.60e-03\n",
      "  30. WGGP_OR_NO           - 3.02e-01 ± 7.18e-03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    if test == 'all':\n",
    "        str_ = 'on all columns'\n",
    "    elif test == 'cont':\n",
    "        str_ = 'on continuous columns'\n",
    "    elif test == 'cat':\n",
    "        str_ = 'on categorical columns'\n",
    "        \n",
    "    for s in ['srmse']:#stats:\n",
    "        print('Ranking {} based on {}:'.format(str_, s.upper()))\n",
    "\n",
    "        if s in ['r2', 'corr']:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "        else:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "        for i, item in enumerate(sorted_dct):\n",
    "            print('  {:>2}. {:<20} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per couple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 2):\n",
    "    combs.append(k[0] + '::' + k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stats for model \u001b[1mCTGAN\u001b[0m (1/29)\n",
      "Preparing stats for model \u001b[1mSGAN_OR_BO\u001b[0m (2/29)\n",
      "Preparing stats for model \u001b[1mSGAN_OR_NO\u001b[0m (3/29)\n",
      "Preparing stats for model \u001b[1mSGAN_OR_OD\u001b[0m (4/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WI_BO\u001b[0m (5/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WI_NO\u001b[0m (6/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WI_OD\u001b[0m (7/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WO_BO\u001b[0m (8/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WO_NO\u001b[0m (9/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WO_OD\u001b[0m (10/29)\n",
      "Preparing stats for model \u001b[1mTGAN\u001b[0m (11/29)\n",
      "Preparing stats for model \u001b[1mWGAN_OR_BO\u001b[0m (12/29)\n",
      "Preparing stats for model \u001b[1mWGAN_OR_NO\u001b[0m (13/29)\n",
      "Preparing stats for model \u001b[1mWGAN_OR_OD\u001b[0m (14/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WI_BO\u001b[0m (15/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WI_NO\u001b[0m (16/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WI_OD\u001b[0m (17/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WO_BO\u001b[0m (18/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WO_NO\u001b[0m (19/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WO_OD\u001b[0m (20/29)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_BO\u001b[0m (21/29)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_NO\u001b[0m (22/29)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_OD\u001b[0m (23/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_BO\u001b[0m (24/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_NO\u001b[0m (25/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_OD\u001b[0m (26/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_BO\u001b[0m (27/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_NO\u001b[0m (28/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_OD\u001b[0m (29/29)\n"
     ]
    }
   ],
   "source": [
    "all_stats = {}\n",
    "\n",
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "    \n",
    "    all_stats[m] = {}\n",
    "    \n",
    "    for c in combs:\n",
    "        all_stats[m][c] = {}\n",
    "        for s in stats_str:\n",
    "            all_stats[m][c][s] = []\n",
    "    \n",
    "    # Load all dataframes for current model\n",
    "    dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "    \n",
    "    # Go through all dataframes generated for each model\n",
    "    for df in dfs:\n",
    "        \n",
    "        # Discretize continuous columns\n",
    "        for c in continuous_cols:\n",
    "            df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "        \n",
    "        # Go through each columns\n",
    "        for c in combs:\n",
    "            \n",
    "            agg_vars = c.split('::')\n",
    "\n",
    "            real = df_orig.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = df.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                all_stats[m][c][s].append(sts[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_orig = {}\n",
    "\n",
    "for c in combs:\n",
    "    stats_orig[c] = {}\n",
    "    for s in stats_str:\n",
    "        stats_orig[c][s] = []\n",
    "\n",
    "for i in range(n_models*n_data):\n",
    "\n",
    "    train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "    train.index = range(len(train))\n",
    "    test = df_orig[~df_orig.index.isin(train.index)]\n",
    "    test.index = range(len(test))\n",
    "\n",
    "    # Go through each columns\n",
    "    for c in combs:\n",
    "\n",
    "        agg_vars = c.split('::')\n",
    "\n",
    "        real = train.copy()\n",
    "        real['count'] = 1\n",
    "        real = real.groupby(agg_vars, observed=True).count()\n",
    "        real /= len(df_orig)\n",
    "\n",
    "        synth = test.copy()\n",
    "        synth['count'] = 1\n",
    "        synth = synth.groupby(agg_vars, observed=True).count()\n",
    "        synth /= len(df)\n",
    "\n",
    "        real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "        real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "        sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "        for s in sts:\n",
    "            stats_orig[c][s].append(sts[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats[orig_str] = stats_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./notebooks/results/couple_combinations.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(all_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./notebooks/results/couple_combinations.json', 'r') as infile:\n",
    "    all_stats = json.loads(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in all_stats.keys():\n",
    "\n",
    "    for s in stats:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all coupled combinations based on SRMSE:\n",
      "   1. random-original      - 1.26e-01 ± 7.43e-03\n",
      "   2. WGAN_WI_NO           - 1.79e-01 ± 1.17e-02\n",
      "   3. WGAN_WI_BO           - 1.93e-01 ± 1.68e-02\n",
      "   4. SGAN_WO_NO           - 1.96e-01 ± 1.12e-02\n",
      "   5. WGAN_WI_OD           - 2.01e-01 ± 1.74e-02\n",
      "   6. SGAN_WO_OD           - 2.02e-01 ± 1.30e-02\n",
      "   7. WGAN_OR_BO           - 2.04e-01 ± 1.25e-02\n",
      "   8. WGAN_OR_OD           - 2.04e-01 ± 1.08e-02\n",
      "   9. SGAN_WI_NO           - 2.08e-01 ± 1.75e-02\n",
      "  10. SGAN_WO_BO           - 2.09e-01 ± 1.64e-02\n",
      "  11. SGAN_WI_OD           - 2.09e-01 ± 1.67e-02\n",
      "  12. SGAN_OR_OD           - 2.10e-01 ± 7.24e-03\n",
      "  13. SGAN_OR_BO           - 2.17e-01 ± 5.73e-03\n",
      "  14. SGAN_WI_BO           - 2.19e-01 ± 1.94e-02\n",
      "  15. WGAN_WO_NO           - 2.21e-01 ± 2.82e-02\n",
      "  16. WGGP_WO_NO           - 2.50e-01 ± 5.13e-02\n",
      "  17. WGAN_WO_OD           - 2.57e-01 ± 3.81e-02\n",
      "  18. WGGP_WO_OD           - 2.61e-01 ± 5.15e-02\n",
      "  19. WGAN_WO_BO           - 2.62e-01 ± 4.57e-02\n",
      "  20. WGGP_WO_BO           - 2.70e-01 ± 6.51e-02\n",
      "  21. TGAN                 - 2.92e-01 ± 4.64e-02\n",
      "  22. WGGP_WI_NO           - 3.09e-01 ± 7.89e-02\n",
      "  23. WGGP_WI_OD           - 3.31e-01 ± 9.82e-02\n",
      "  24. WGGP_WI_BO           - 3.53e-01 ± 1.24e-01\n",
      "  25. WGGP_OR_OD           - 4.38e-01 ± 1.30e-01\n",
      "  26. WGGP_OR_BO           - 4.42e-01 ± 1.35e-01\n",
      "  27. CTGAN                - 5.56e-01 ± 3.62e-02\n",
      "  28. SGAN_OR_NO           - 6.15e-01 ± 9.78e-03\n",
      "  29. WGAN_OR_NO           - 6.19e-01 ± 1.13e-02\n",
      "  30. WGGP_OR_NO           - 7.00e-01 ± 7.99e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats:\n",
    "    print('Ranking on all coupled combinations based on {}:'.format(s.upper()))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<20} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per trouple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 3):\n",
    "    combs.append(k[0] + '::' + k[1] + '::' + k[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stats for model \u001b[1mCTGAN\u001b[0m (1/29)\n",
      "Preparing stats for model \u001b[1mSGAN_OR_BO\u001b[0m (2/29)\n",
      "Preparing stats for model \u001b[1mSGAN_OR_NO\u001b[0m (3/29)\n",
      "Preparing stats for model \u001b[1mSGAN_OR_OD\u001b[0m (4/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WI_BO\u001b[0m (5/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WI_NO\u001b[0m (6/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WI_OD\u001b[0m (7/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WO_BO\u001b[0m (8/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WO_NO\u001b[0m (9/29)\n",
      "Preparing stats for model \u001b[1mSGAN_WO_OD\u001b[0m (10/29)\n",
      "Preparing stats for model \u001b[1mTGAN\u001b[0m (11/29)\n",
      "Preparing stats for model \u001b[1mWGAN_OR_BO\u001b[0m (12/29)\n",
      "Preparing stats for model \u001b[1mWGAN_OR_NO\u001b[0m (13/29)\n",
      "Preparing stats for model \u001b[1mWGAN_OR_OD\u001b[0m (14/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WI_BO\u001b[0m (15/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WI_NO\u001b[0m (16/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WI_OD\u001b[0m (17/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WO_BO\u001b[0m (18/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WO_NO\u001b[0m (19/29)\n",
      "Preparing stats for model \u001b[1mWGAN_WO_OD\u001b[0m (20/29)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_BO\u001b[0m (21/29)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_NO\u001b[0m (22/29)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_OD\u001b[0m (23/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_BO\u001b[0m (24/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_NO\u001b[0m (25/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_OD\u001b[0m (26/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_BO\u001b[0m (27/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_NO\u001b[0m (28/29)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_OD\u001b[0m (29/29)\n"
     ]
    }
   ],
   "source": [
    "all_stats = {}\n",
    "\n",
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "    \n",
    "    all_stats[m] = {}\n",
    "    \n",
    "    for c in combs:\n",
    "        all_stats[m][c] = {}\n",
    "        for s in stats_str:\n",
    "            all_stats[m][c][s] = []\n",
    "    \n",
    "    # Load all dataframes for current model\n",
    "    dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "    \n",
    "    # Go through all dataframes generated for each model\n",
    "    for df in dfs:\n",
    "        \n",
    "        # Discretize continuous columns\n",
    "        for c in continuous_cols:\n",
    "            df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "        \n",
    "        # Go through each columns\n",
    "        for c in combs:\n",
    "            \n",
    "            agg_vars = c.split('::')\n",
    "\n",
    "            real = df_orig.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = df.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                all_stats[m][c][s].append(sts[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_orig = {}\n",
    "\n",
    "for c in combs:\n",
    "    stats_orig[c] = {}\n",
    "    for s in stats_str:\n",
    "        stats_orig[c][s] = []\n",
    "\n",
    "for i in range(n_models*n_data):\n",
    "\n",
    "    train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "    train.index = range(len(train))\n",
    "    test = df_orig[~df_orig.index.isin(train.index)]\n",
    "    test.index = range(len(test))\n",
    "\n",
    "    # Go through each columns\n",
    "    for c in combs:\n",
    "\n",
    "        agg_vars = c.split('::')\n",
    "\n",
    "        real = train.copy()\n",
    "        real['count'] = 1\n",
    "        real = real.groupby(agg_vars, observed=True).count()\n",
    "        real /= len(df_orig)\n",
    "\n",
    "        synth = test.copy()\n",
    "        synth['count'] = 1\n",
    "        synth = synth.groupby(agg_vars, observed=True).count()\n",
    "        synth /= len(df)\n",
    "\n",
    "        real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "        real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "        sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "        for s in sts:\n",
    "            stats_orig[c][s].append(sts[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats[orig_str] = stats_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./notebooks/results/trouple_combinations.json', 'w') as outfile:\n",
    "    outfile.write(json.dumps(all_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./notebooks/results/trouple_combinations.json', 'r') as infile:\n",
    "    all_stats = json.loads(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in all_stats.keys():\n",
    "\n",
    "    for s in stats:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all triple combinations based on SRMSE:\n",
      "   1. random-original      - 2.42e-01 ± 6.47e-03\n",
      "   2. WGAN_WI_NO           - 3.65e-01 ± 1.84e-02\n",
      "   3. WGAN_WI_BO           - 3.70e-01 ± 2.52e-02\n",
      "   4. WGAN_WI_OD           - 3.80e-01 ± 2.27e-02\n",
      "   5. SGAN_WO_NO           - 4.01e-01 ± 2.07e-02\n",
      "   6. WGAN_OR_OD           - 4.04e-01 ± 1.47e-02\n",
      "   7. WGAN_OR_BO           - 4.05e-01 ± 1.73e-02\n",
      "   8. SGAN_WO_OD           - 4.09e-01 ± 2.47e-02\n",
      "   9. SGAN_OR_OD           - 4.15e-01 ± 1.44e-02\n",
      "  10. SGAN_WO_BO           - 4.21e-01 ± 3.11e-02\n",
      "  11. SGAN_WI_NO           - 4.23e-01 ± 2.54e-02\n",
      "  12. SGAN_WI_OD           - 4.24e-01 ± 2.34e-02\n",
      "  13. SGAN_OR_BO           - 4.28e-01 ± 1.58e-02\n",
      "  14. SGAN_WI_BO           - 4.40e-01 ± 2.65e-02\n",
      "  15. WGAN_WO_NO           - 4.62e-01 ± 5.41e-02\n",
      "  16. WGGP_WO_NO           - 4.81e-01 ± 9.03e-02\n",
      "  17. WGGP_WO_OD           - 4.83e-01 ± 8.72e-02\n",
      "  18. WGAN_WO_OD           - 4.83e-01 ± 7.46e-02\n",
      "  19. WGAN_WO_BO           - 4.92e-01 ± 8.49e-02\n",
      "  20. WGGP_WO_BO           - 4.96e-01 ± 1.10e-01\n",
      "  21. TGAN                 - 5.43e-01 ± 7.04e-02\n",
      "  22. WGGP_WI_NO           - 5.77e-01 ± 1.27e-01\n",
      "  23. WGGP_WI_OD           - 5.95e-01 ± 1.54e-01\n",
      "  24. WGGP_WI_BO           - 6.26e-01 ± 1.93e-01\n",
      "  25. WGGP_OR_OD           - 7.87e-01 ± 2.35e-01\n",
      "  26. WGGP_OR_BO           - 7.91e-01 ± 2.46e-01\n",
      "  27. CTGAN                - 9.19e-01 ± 5.08e-02\n",
      "  28. SGAN_OR_NO           - 1.13e+00 ± 1.82e-02\n",
      "  29. WGAN_OR_NO           - 1.14e+00 ± 1.88e-02\n",
      "  30. WGGP_OR_NO           - 1.25e+00 ± 1.30e-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats:\n",
    "    print('Ranking on all triple combinations based on {}:'.format(s.upper()))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<20} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
