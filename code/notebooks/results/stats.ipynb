{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\glede\\anaconda3\\envs\\py37\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Iterable, defaultdict\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For the Python notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_DATGAN(name):\n",
    "    if any(x in name for x in ['TGAN', 'CTGAN', 'TVAE', 'FULL', 'TRANSRED', 'LINEAR', 'NOLINKS', 'PREDICTION']):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def compute_stats(freq_list_orig, freq_list_synth):\n",
    "    \"\"\"\n",
    "    Different statistics computed on the frequency list\n",
    "    \n",
    "    \"\"\"\n",
    "    freq_list_orig, freq_list_synth = np.array(freq_list_orig), np.array(freq_list_synth)\n",
    "    corr_mat = np.corrcoef(freq_list_orig, freq_list_synth)\n",
    "    corr = corr_mat[0, 1]\n",
    "    if np.isnan(corr): corr = 0.0\n",
    "    # MAE\n",
    "    mae = np.absolute(freq_list_orig - freq_list_synth).mean()\n",
    "    # RMSE\n",
    "    rmse = np.linalg.norm(freq_list_orig - freq_list_synth) / np.sqrt(len(freq_list_orig))\n",
    "    # SRMSE\n",
    "    freq_list_orig_avg = freq_list_orig.mean()\n",
    "    srmse = rmse / freq_list_orig_avg\n",
    "    # r-square\n",
    "    u = np.sum((freq_list_synth - freq_list_orig)**2)\n",
    "    v = np.sum((freq_list_orig - freq_list_orig_avg)**2)\n",
    "    r2 = 1.0 - u / v\n",
    "    stat = {'mae': mae, 'rmse': rmse, 'r2': r2, 'srmse': srmse, 'corr': corr}\n",
    "    \n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all models and associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'LPMC'\n",
    "n_models = 5\n",
    "n_data = 5\n",
    "\n",
    "# Models for testing all DATGANS\n",
    "models = ['CTGAN', 'TGAN', 'TVAE']\n",
    "\n",
    "for i in ['WGAN', 'SGAN', 'WGGP']:\n",
    "    for j in ['WI', 'OR', 'WO']:\n",
    "        for k in ['NO', 'BO', 'OD', 'OC']:\n",
    "            models.append('{}_{}_{}'.format(i,j,k))\n",
    "            \n",
    "# Models for testing different DAGs\n",
    "if 'DAG' in dataset:\n",
    "    models = ['FULL', 'TRANSRED', 'LINEAR', 'NOLINKS', 'PREDICTION']\n",
    "            \n",
    "models.sort()\n",
    "\n",
    "files_ = {}\n",
    "\n",
    "for m in models:\n",
    "    tmp = []\n",
    "    if is_a_DATGAN(m):\n",
    "        spl = m.split('_')\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append('{}_{}_{:0>2}_{}_{:0>2}.csv'.format(spl[0], spl[1], i+1,  spl[2], j+1))\n",
    "    else:\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append('{}_{:0>2}_{:0>2}.csv'.format(m, i+1, j+1))\n",
    "    files_[m] = tmp\n",
    "\n",
    "\n",
    "input_folder = '../synth_data/{}/'.format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('../data/' + dataset.split('_')[0] + '/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Chicago' in dataset:\n",
    "    continuous_cols = ['distance', 'age', 'departure_time']\n",
    "elif 'LPMC' in dataset:\n",
    "    continuous_cols = ['start_time_linear', 'age', 'distance', 'dur_walking', 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', 'dur_pt_int', 'dur_driving', 'cost_transit', 'cost_driving_fuel', 'driving_traffic_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_cont = {}\n",
    "\n",
    "for c in continuous_cols:\n",
    "    #bins_cont[c] = pd.qcut(df_orig[c], q=10, retbins=True)[1]\n",
    "    bins_cont[c] = pd.cut(df_orig[c], bins=10, retbins=True)[1]\n",
    "    bins_cont[c][0] = -np.inf\n",
    "    bins_cont[c][-1] = np.inf\n",
    "    df_orig[c] = pd.cut(df_orig[c], bins=bins_cont[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>travel_mode</th>\n",
       "      <th>purpose</th>\n",
       "      <th>fueltype</th>\n",
       "      <th>faretype</th>\n",
       "      <th>bus_scale</th>\n",
       "      <th>travel_year</th>\n",
       "      <th>travel_month</th>\n",
       "      <th>travel_date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>start_time_linear</th>\n",
       "      <th>...</th>\n",
       "      <th>dur_pt_access</th>\n",
       "      <th>dur_pt_rail</th>\n",
       "      <th>dur_pt_bus</th>\n",
       "      <th>dur_pt_int</th>\n",
       "      <th>pt_n_interchanges</th>\n",
       "      <th>dur_driving</th>\n",
       "      <th>cost_transit</th>\n",
       "      <th>cost_driving_fuel</th>\n",
       "      <th>cost_driving_con_charge</th>\n",
       "      <th>driving_traffic_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drive</td>\n",
       "      <td>HBO</td>\n",
       "      <td>Petrol_Car</td>\n",
       "      <td>child</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>(9.567, 11.958]</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.106, 0.212]</td>\n",
       "      <td>(-inf, 0.137]</td>\n",
       "      <td>(-inf, 0.215]</td>\n",
       "      <td>(-inf, 0.0567]</td>\n",
       "      <td>0</td>\n",
       "      <td>(-inf, 0.183]</td>\n",
       "      <td>(-inf, 1.17]</td>\n",
       "      <td>(-inf, 1.027]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.104, 0.208]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drive</td>\n",
       "      <td>HBO</td>\n",
       "      <td>Petrol_Car</td>\n",
       "      <td>free</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>(16.742, 19.133]</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.212, 0.318]</td>\n",
       "      <td>(-inf, 0.137]</td>\n",
       "      <td>(-inf, 0.215]</td>\n",
       "      <td>(-inf, 0.0567]</td>\n",
       "      <td>0</td>\n",
       "      <td>(-inf, 0.183]</td>\n",
       "      <td>(-inf, 1.17]</td>\n",
       "      <td>(-inf, 1.027]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(-inf, 0.104]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drive</td>\n",
       "      <td>HBO</td>\n",
       "      <td>Petrol_Car</td>\n",
       "      <td>full</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>(11.958, 14.35]</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.212, 0.318]</td>\n",
       "      <td>(-inf, 0.137]</td>\n",
       "      <td>(0.859, 1.074]</td>\n",
       "      <td>(0.0567, 0.113]</td>\n",
       "      <td>1</td>\n",
       "      <td>(0.362, 0.54]</td>\n",
       "      <td>(2.34, 3.51]</td>\n",
       "      <td>(1.027, 2.034]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(0.313, 0.417]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pt</td>\n",
       "      <td>HBW</td>\n",
       "      <td>Average_Car</td>\n",
       "      <td>full</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>(19.133, 21.525]</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.106, 0.212]</td>\n",
       "      <td>(-inf, 0.137]</td>\n",
       "      <td>(-inf, 0.215]</td>\n",
       "      <td>(0.0567, 0.113]</td>\n",
       "      <td>1</td>\n",
       "      <td>(-inf, 0.183]</td>\n",
       "      <td>(2.34, 3.51]</td>\n",
       "      <td>(-inf, 1.027]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(-inf, 0.104]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pt</td>\n",
       "      <td>HBO</td>\n",
       "      <td>Average_Car</td>\n",
       "      <td>free</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>(7.175, 9.567]</td>\n",
       "      <td>...</td>\n",
       "      <td>(0.106, 0.212]</td>\n",
       "      <td>(-inf, 0.137]</td>\n",
       "      <td>(0.215, 0.429]</td>\n",
       "      <td>(-inf, 0.0567]</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.183, 0.362]</td>\n",
       "      <td>(-inf, 1.17]</td>\n",
       "      <td>(-inf, 1.027]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(-inf, 0.104]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  travel_mode purpose     fueltype faretype  bus_scale  travel_year  \\\n",
       "0       drive     HBO   Petrol_Car    child        0.0         2012   \n",
       "1       drive     HBO   Petrol_Car     free        0.0         2012   \n",
       "2       drive     HBO   Petrol_Car     full        1.0         2012   \n",
       "3          pt     HBW  Average_Car     full        1.0         2012   \n",
       "4          pt     HBO  Average_Car     free        0.0         2012   \n",
       "\n",
       "   travel_month  travel_date  day_of_week start_time_linear  ...  \\\n",
       "0             4            1            7   (9.567, 11.958]  ...   \n",
       "1             4            1            7  (16.742, 19.133]  ...   \n",
       "2             4            1            7   (11.958, 14.35]  ...   \n",
       "3             4            1            7  (19.133, 21.525]  ...   \n",
       "4             4            1            7    (7.175, 9.567]  ...   \n",
       "\n",
       "    dur_pt_access    dur_pt_rail      dur_pt_bus       dur_pt_int  \\\n",
       "0  (0.106, 0.212]  (-inf, 0.137]   (-inf, 0.215]   (-inf, 0.0567]   \n",
       "1  (0.212, 0.318]  (-inf, 0.137]   (-inf, 0.215]   (-inf, 0.0567]   \n",
       "2  (0.212, 0.318]  (-inf, 0.137]  (0.859, 1.074]  (0.0567, 0.113]   \n",
       "3  (0.106, 0.212]  (-inf, 0.137]   (-inf, 0.215]  (0.0567, 0.113]   \n",
       "4  (0.106, 0.212]  (-inf, 0.137]  (0.215, 0.429]   (-inf, 0.0567]   \n",
       "\n",
       "  pt_n_interchanges     dur_driving  cost_transit cost_driving_fuel  \\\n",
       "0                 0   (-inf, 0.183]  (-inf, 1.17]     (-inf, 1.027]   \n",
       "1                 0   (-inf, 0.183]  (-inf, 1.17]     (-inf, 1.027]   \n",
       "2                 1   (0.362, 0.54]  (2.34, 3.51]    (1.027, 2.034]   \n",
       "3                 1   (-inf, 0.183]  (2.34, 3.51]     (-inf, 1.027]   \n",
       "4                 0  (0.183, 0.362]  (-inf, 1.17]     (-inf, 1.027]   \n",
       "\n",
       "  cost_driving_con_charge driving_traffic_percent  \n",
       "0                     0.0          (0.104, 0.208]  \n",
       "1                     0.0           (-inf, 0.104]  \n",
       "2                     0.0          (0.313, 0.417]  \n",
       "3                     0.0           (-inf, 0.104]  \n",
       "4                     0.0           (-inf, 0.104]  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_str = ['mae', 'rmse', 'r2', 'srmse', 'corr']\n",
    "orig_str = 'random-original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('./notebooks/results/{}'.format(dataset))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per individual column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous pickel file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'single_columns.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mCTGAN\u001b[0m (1/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_BO\u001b[0m (2/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_NO\u001b[0m (3/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_OC\u001b[0m (4/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_OD\u001b[0m (5/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_BO\u001b[0m (6/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_NO\u001b[0m (7/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_OC\u001b[0m (8/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_OD\u001b[0m (9/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_BO\u001b[0m (10/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_NO\u001b[0m (11/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_OC\u001b[0m (12/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_OD\u001b[0m (13/39) already exists!\n",
      "Results for model \u001b[1mTGAN\u001b[0m (14/39) already exists!\n",
      "Results for model \u001b[1mTVAE\u001b[0m (15/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_BO\u001b[0m (16/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_NO\u001b[0m (17/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_OC\u001b[0m (18/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_OD\u001b[0m (19/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_BO\u001b[0m (20/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_NO\u001b[0m (21/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_OC\u001b[0m (22/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_OD\u001b[0m (23/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_BO\u001b[0m (24/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_NO\u001b[0m (25/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_OC\u001b[0m (26/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_OD\u001b[0m (27/39) already exists!\n",
      "Preparing stats for model \u001b[1mWGGP_OR_BO\u001b[0m (28/39)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_NO\u001b[0m (29/39)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_OC\u001b[0m (30/39)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_OD\u001b[0m (31/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_BO\u001b[0m (32/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_NO\u001b[0m (33/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_OC\u001b[0m (34/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_OD\u001b[0m (35/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_BO\u001b[0m (36/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_NO\u001b[0m (37/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_OC\u001b[0m (38/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_OD\u001b[0m (39/39)\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        for c in df_orig.columns:\n",
    "            all_stats[m][c] = {}\n",
    "            for s in stats_str:\n",
    "                all_stats[m][c][s] = []\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "\n",
    "        # Go through all dataframes generated for each model\n",
    "        for df in dfs:\n",
    "\n",
    "            # Discretize continuous columns\n",
    "            for c in continuous_cols:\n",
    "                df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "            # Go through each columns\n",
    "            for c in df_orig.columns:\n",
    "\n",
    "                agg_vars = [c]\n",
    "\n",
    "                real = df_orig.copy()\n",
    "                real['count'] = 1\n",
    "                real = real.groupby(agg_vars, observed=True).count()\n",
    "                real /= len(df_orig)\n",
    "\n",
    "                synth = df.copy()\n",
    "                synth['count'] = 1\n",
    "                synth = synth.groupby(agg_vars, observed=True).count()\n",
    "                synth /= len(df)\n",
    "\n",
    "                real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "                real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "                sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "                for s in sts:\n",
    "                    all_stats[m][c][s].append(sts[s])\n",
    "                    \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in all_stats:\n",
    "\n",
    "    stats_orig = {}\n",
    "\n",
    "    for c in df_orig.columns:\n",
    "        stats_orig[c] = {}\n",
    "        for s in stats_str:\n",
    "            stats_orig[c][s] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in df_orig.columns:\n",
    "\n",
    "            agg_vars = [c]\n",
    "\n",
    "            real = train.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = test.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                stats_orig[c][s].append(sts[s])\n",
    "    \n",
    "    all_stats[orig_str] = stats_orig\n",
    "    \n",
    "    pickle.dump(all_stats, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    res[test] = {}\n",
    "    \n",
    "    if test == 'all':\n",
    "        cols = df_orig.columns\n",
    "    elif test == 'cont':\n",
    "        cols = continuous_cols\n",
    "    elif test == 'cat':\n",
    "        cols = set(df_orig.columns) - set(continuous_cols)\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[test][s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "\n",
    "        for s in stats_str:\n",
    "            res[test][s][m] = []\n",
    "\n",
    "            for i in range(n_models*n_data):\n",
    "                tmp = []\n",
    "\n",
    "                for c in cols:\n",
    "                    tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "                res[test][s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    avg[test] = {}\n",
    "\n",
    "    for s in stats_str:\n",
    "        avg[test][s] = {}\n",
    "\n",
    "        for m in all_stats.keys():\n",
    "            avg[test][s][m] = {\n",
    "                'mean': np.mean(res[test][s][m]),\n",
    "                'std': np.std(res[test][s][m])\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all columns based on SRMSE:\n",
      "   1. random-original - 7.50e-02 ± 4.56e-03\n",
      "   2. WGGP_WI_NO      - 8.13e-02 ± 6.37e-03\n",
      "   3. WGGP_WI_OC      - 8.77e-02 ± 6.71e-03\n",
      "   4. SGAN_WI_NO      - 8.99e-02 ± 4.44e-03\n",
      "   5. SGAN_WI_OC      - 9.24e-02 ± 4.40e-03\n",
      "   6. WGGP_WO_OC      - 9.53e-02 ± 8.33e-03\n",
      "   7. SGAN_WI_OD      - 9.75e-02 ± 5.94e-03\n",
      "   8. WGGP_WI_OD      - 9.77e-02 ± 6.90e-03\n",
      "   9. WGGP_WO_NO      - 1.00e-01 ± 5.68e-03\n",
      "  10. SGAN_WI_BO      - 1.00e-01 ± 8.56e-03\n",
      "  11. WGGP_WO_BO      - 1.00e-01 ± 8.35e-03\n",
      "  12. WGGP_WI_BO      - 1.04e-01 ± 6.56e-03\n",
      "  13. WGGP_WO_OD      - 1.05e-01 ± 6.28e-03\n",
      "  14. WGAN_WI_NO      - 1.12e-01 ± 5.69e-03\n",
      "  15. SGAN_WO_NO      - 1.13e-01 ± 1.08e-02\n",
      "  16. WGGP_OR_OD      - 1.15e-01 ± 1.03e-02\n",
      "  17. WGAN_WI_OC      - 1.18e-01 ± 7.46e-03\n",
      "  18. WGGP_OR_BO      - 1.20e-01 ± 8.12e-03\n",
      "  19. SGAN_WO_OC      - 1.22e-01 ± 9.72e-03\n",
      "  20. SGAN_WO_OD      - 1.27e-01 ± 1.26e-02\n",
      "  21. SGAN_OR_OD      - 1.27e-01 ± 1.94e-02\n",
      "  22. WGAN_WO_NO      - 1.33e-01 ± 1.42e-02\n",
      "  23. SGAN_WO_BO      - 1.36e-01 ± 9.50e-03\n",
      "  24. SGAN_OR_BO      - 1.43e-01 ± 1.48e-02\n",
      "  25. TGAN            - 1.48e-01 ± 1.10e-02\n",
      "  26. WGAN_OR_OD      - 1.48e-01 ± 1.83e-02\n",
      "  27. WGGP_OR_NO      - 1.58e-01 ± 5.43e-03\n",
      "  28. WGGP_OR_OC      - 1.62e-01 ± 5.25e-03\n",
      "  29. WGAN_OR_BO      - 1.63e-01 ± 1.28e-02\n",
      "  30. WGAN_WO_OC      - 1.65e-01 ± 9.03e-03\n",
      "  31. SGAN_OR_NO      - 1.66e-01 ± 1.61e-02\n",
      "  32. WGAN_OR_NO      - 1.72e-01 ± 1.51e-02\n",
      "  33. WGAN_WI_OD      - 1.74e-01 ± 4.67e-03\n",
      "  34. WGAN_WI_BO      - 1.79e-01 ± 8.08e-03\n",
      "  35. SGAN_OR_OC      - 1.81e-01 ± 1.45e-02\n",
      "  36. WGAN_OR_OC      - 1.89e-01 ± 9.88e-03\n",
      "  37. WGAN_WO_OD      - 1.98e-01 ± 1.26e-02\n",
      "  38. CTGAN           - 2.27e-01 ± 1.62e-02\n",
      "  39. WGAN_WO_BO      - 2.30e-01 ± 9.50e-03\n",
      "  40. TVAE            - 2.76e-01 ± 1.31e-02\n",
      "\n",
      "Ranking on continuous columns based on SRMSE:\n",
      "   1. random-original - 3.41e-02 ± 6.64e-03\n",
      "   2. WGGP_WI_OD      - 1.31e-01 ± 1.15e-02\n",
      "   3. WGGP_WI_NO      - 1.31e-01 ± 1.26e-02\n",
      "   4. WGGP_OR_OD      - 1.40e-01 ± 1.36e-02\n",
      "   5. WGGP_OR_NO      - 1.40e-01 ± 1.24e-02\n",
      "   6. WGGP_WI_OC      - 1.42e-01 ± 1.30e-02\n",
      "   7. WGGP_WI_BO      - 1.43e-01 ± 1.22e-02\n",
      "   8. SGAN_WI_OD      - 1.46e-01 ± 9.38e-03\n",
      "   9. SGAN_WI_NO      - 1.47e-01 ± 9.40e-03\n",
      "  10. WGGP_OR_OC      - 1.50e-01 ± 1.06e-02\n",
      "  11. WGGP_OR_BO      - 1.50e-01 ± 1.03e-02\n",
      "  12. SGAN_WI_BO      - 1.51e-01 ± 1.42e-02\n",
      "  13. SGAN_WI_OC      - 1.52e-01 ± 1.34e-02\n",
      "  14. SGAN_OR_NO      - 1.58e-01 ± 3.28e-02\n",
      "  15. SGAN_OR_OD      - 1.58e-01 ± 3.32e-02\n",
      "  16. WGGP_WO_BO      - 1.67e-01 ± 1.65e-02\n",
      "  17. WGGP_WO_OC      - 1.67e-01 ± 1.59e-02\n",
      "  18. WGAN_OR_NO      - 1.69e-01 ± 3.21e-02\n",
      "  19. WGAN_OR_OD      - 1.72e-01 ± 3.23e-02\n",
      "  20. WGGP_WO_OD      - 1.76e-01 ± 1.17e-02\n",
      "  21. WGGP_WO_NO      - 1.77e-01 ± 1.08e-02\n",
      "  22. SGAN_OR_OC      - 1.90e-01 ± 2.94e-02\n",
      "  23. SGAN_OR_BO      - 1.91e-01 ± 2.96e-02\n",
      "  24. SGAN_WO_OD      - 1.96e-01 ± 1.99e-02\n",
      "  25. SGAN_WO_NO      - 1.97e-01 ± 2.00e-02\n",
      "  26. WGAN_OR_OC      - 2.03e-01 ± 2.08e-02\n",
      "  27. WGAN_OR_BO      - 2.04e-01 ± 2.06e-02\n",
      "  28. WGAN_WI_NO      - 2.06e-01 ± 9.82e-03\n",
      "  29. WGAN_WI_OD      - 2.06e-01 ± 1.01e-02\n",
      "  30. SGAN_WO_OC      - 2.16e-01 ± 1.86e-02\n",
      "  31. SGAN_WO_BO      - 2.16e-01 ± 1.70e-02\n",
      "  32. WGAN_WI_BO      - 2.17e-01 ± 1.59e-02\n",
      "  33. WGAN_WI_OC      - 2.18e-01 ± 1.55e-02\n",
      "  34. CTGAN           - 2.35e-01 ± 3.62e-02\n",
      "  35. TGAN            - 2.46e-01 ± 1.88e-02\n",
      "  36. WGAN_WO_NO      - 2.50e-01 ± 3.04e-02\n",
      "  37. WGAN_WO_OD      - 2.51e-01 ± 3.16e-02\n",
      "  38. TVAE            - 2.79e-01 ± 2.51e-02\n",
      "  39. WGAN_WO_OC      - 3.17e-01 ± 1.65e-02\n",
      "  40. WGAN_WO_BO      - 3.18e-01 ± 1.41e-02\n",
      "\n",
      "Ranking on categorical columns based on SRMSE:\n",
      "   1. WGAN_WO_NO      - 2.42e-02 ± 4.72e-03\n",
      "   2. WGAN_WO_OC      - 2.45e-02 ± 4.15e-03\n",
      "   3. WGAN_WI_OC      - 2.48e-02 ± 3.55e-03\n",
      "   4. WGAN_WI_NO      - 2.50e-02 ± 4.20e-03\n",
      "   5. WGGP_WO_OC      - 2.83e-02 ± 2.97e-03\n",
      "   6. WGGP_WO_NO      - 2.86e-02 ± 3.98e-03\n",
      "   7. SGAN_WO_OC      - 3.43e-02 ± 3.19e-03\n",
      "   8. WGGP_WI_NO      - 3.52e-02 ± 3.41e-03\n",
      "   9. SGAN_WO_NO      - 3.54e-02 ± 2.92e-03\n",
      "  10. WGGP_WI_OC      - 3.70e-02 ± 4.15e-03\n",
      "  11. SGAN_WI_NO      - 3.71e-02 ± 5.37e-03\n",
      "  12. SGAN_WI_OC      - 3.75e-02 ± 5.60e-03\n",
      "  13. WGGP_WO_BO      - 3.77e-02 ± 2.94e-03\n",
      "  14. WGGP_WO_OD      - 3.77e-02 ± 2.97e-03\n",
      "  15. SGAN_WI_OD      - 5.20e-02 ± 7.58e-03\n",
      "  16. SGAN_WI_BO      - 5.26e-02 ± 8.14e-03\n",
      "  17. TGAN            - 5.69e-02 ± 6.38e-03\n",
      "  18. SGAN_WO_BO      - 6.17e-02 ± 8.89e-03\n",
      "  19. SGAN_WO_OD      - 6.24e-02 ± 8.61e-03\n",
      "  20. WGGP_WI_OD      - 6.70e-02 ± 6.19e-03\n",
      "  21. WGGP_WI_BO      - 6.80e-02 ± 5.76e-03\n",
      "  22. WGGP_OR_BO      - 9.11e-02 ± 7.00e-03\n",
      "  23. WGGP_OR_OD      - 9.13e-02 ± 7.69e-03\n",
      "  24. SGAN_OR_OD      - 9.79e-02 ± 1.02e-02\n",
      "  25. SGAN_OR_BO      - 9.85e-02 ± 9.66e-03\n",
      "  26. random-original - 1.13e-01 ± 3.35e-03\n",
      "  27. WGAN_OR_BO      - 1.26e-01 ± 6.80e-03\n",
      "  28. WGAN_OR_OD      - 1.26e-01 ± 7.03e-03\n",
      "  29. WGAN_WI_BO      - 1.44e-01 ± 4.09e-03\n",
      "  30. WGAN_WI_OD      - 1.44e-01 ± 4.11e-03\n",
      "  31. WGAN_WO_BO      - 1.49e-01 ± 9.22e-03\n",
      "  32. WGAN_WO_OD      - 1.50e-01 ± 8.33e-03\n",
      "  33. WGGP_OR_OC      - 1.72e-01 ± 5.02e-03\n",
      "  34. WGGP_OR_NO      - 1.73e-01 ± 4.60e-03\n",
      "  35. SGAN_OR_OC      - 1.73e-01 ± 4.59e-03\n",
      "  36. SGAN_OR_NO      - 1.74e-01 ± 4.55e-03\n",
      "  37. WGAN_OR_NO      - 1.74e-01 ± 2.98e-03\n",
      "  38. WGAN_OR_OC      - 1.75e-01 ± 2.96e-03\n",
      "  39. CTGAN           - 2.18e-01 ± 1.57e-02\n",
      "  40. TVAE            - 2.74e-01 ± 1.38e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    if test == 'all':\n",
    "        str_ = 'on all columns'\n",
    "    elif test == 'cont':\n",
    "        str_ = 'on continuous columns'\n",
    "    elif test == 'cat':\n",
    "        str_ = 'on categorical columns'\n",
    "        \n",
    "    for s in ['srmse']:#stats:\n",
    "        print('Ranking {} based on {}:'.format(str_, s.upper()))\n",
    "\n",
    "        if s in ['r2', 'corr']:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "        else:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "        for i, item in enumerate(sorted_dct):\n",
    "            print('  {:>2}. {:<15} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per couple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 2):\n",
    "    combs.append(k[0] + '::' + k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous pickel file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'couple_combinations.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mCTGAN\u001b[0m (1/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_BO\u001b[0m (2/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_NO\u001b[0m (3/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_OC\u001b[0m (4/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_OD\u001b[0m (5/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_BO\u001b[0m (6/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_NO\u001b[0m (7/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_OC\u001b[0m (8/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_OD\u001b[0m (9/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_BO\u001b[0m (10/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_NO\u001b[0m (11/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_OC\u001b[0m (12/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_OD\u001b[0m (13/39) already exists!\n",
      "Results for model \u001b[1mTGAN\u001b[0m (14/39) already exists!\n",
      "Results for model \u001b[1mTVAE\u001b[0m (15/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_BO\u001b[0m (16/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_NO\u001b[0m (17/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_OC\u001b[0m (18/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_OD\u001b[0m (19/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_BO\u001b[0m (20/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_NO\u001b[0m (21/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_OC\u001b[0m (22/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_OD\u001b[0m (23/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_BO\u001b[0m (24/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_NO\u001b[0m (25/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_OC\u001b[0m (26/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_OD\u001b[0m (27/39) already exists!\n",
      "Preparing stats for model \u001b[1mWGGP_OR_BO\u001b[0m (28/39)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_NO\u001b[0m (29/39)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_OC\u001b[0m (30/39)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_OD\u001b[0m (31/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_BO\u001b[0m (32/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_NO\u001b[0m (33/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_OC\u001b[0m (34/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_OD\u001b[0m (35/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_BO\u001b[0m (36/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_NO\u001b[0m (37/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_OC\u001b[0m (38/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_OD\u001b[0m (39/39)\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "    \n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        for c in combs:\n",
    "            all_stats[m][c] = {}\n",
    "            for s in stats_str:\n",
    "                all_stats[m][c][s] = []\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "\n",
    "        # Go through all dataframes generated for each model\n",
    "        for df in dfs:\n",
    "\n",
    "            # Discretize continuous columns\n",
    "            for c in continuous_cols:\n",
    "                df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "            # Go through each columns\n",
    "            for c in combs:\n",
    "\n",
    "                agg_vars = c.split('::')\n",
    "\n",
    "                real = df_orig.copy()\n",
    "                real['count'] = 1\n",
    "                real = real.groupby(agg_vars, observed=True).count()\n",
    "                real /= len(df_orig)\n",
    "\n",
    "                synth = df.copy()\n",
    "                synth['count'] = 1\n",
    "                synth = synth.groupby(agg_vars, observed=True).count()\n",
    "                synth /= len(df)\n",
    "\n",
    "                real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "                real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "                sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "                for s in sts:\n",
    "                    all_stats[m][c][s].append(sts[s])\n",
    "                    \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in all_stats:\n",
    "    stats_orig = {}\n",
    "\n",
    "    for c in combs:\n",
    "        stats_orig[c] = {}\n",
    "        for s in stats_str:\n",
    "            stats_orig[c][s] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in combs:\n",
    "\n",
    "            agg_vars = c.split('::')\n",
    "\n",
    "            real = train.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = test.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                stats_orig[c][s].append(sts[s])\n",
    "                \n",
    "    all_stats[orig_str] = stats_orig\n",
    "    \n",
    "    pickle.dump(all_stats, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in all_stats.keys():\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all coupled combinations based on SRMSE:\n",
      "   1. random-original - 2.02e-01 ± 4.69e-03\n",
      "   2. WGGP_WI_NO      - 2.26e-01 ± 9.03e-03\n",
      "   3. WGGP_WI_OC      - 2.35e-01 ± 1.30e-02\n",
      "   4. WGGP_WI_OD      - 2.71e-01 ± 1.25e-02\n",
      "   5. WGGP_WO_OC      - 2.75e-01 ± 1.18e-02\n",
      "   6. WGGP_WO_NO      - 2.77e-01 ± 1.69e-02\n",
      "   7. WGGP_WI_BO      - 2.77e-01 ± 1.69e-02\n",
      "   8. SGAN_WI_NO      - 2.78e-01 ± 1.67e-02\n",
      "   9. SGAN_WI_OC      - 2.79e-01 ± 1.05e-02\n",
      "  10. SGAN_WI_OD      - 2.82e-01 ± 1.63e-02\n",
      "  11. WGGP_WO_BO      - 2.85e-01 ± 1.36e-02\n",
      "  12. SGAN_WI_BO      - 2.85e-01 ± 1.48e-02\n",
      "  13. WGGP_WO_OD      - 2.86e-01 ± 1.63e-02\n",
      "  14. WGGP_OR_OD      - 2.88e-01 ± 1.64e-02\n",
      "  15. WGGP_OR_BO      - 2.97e-01 ± 2.12e-02\n",
      "  16. SGAN_WO_NO      - 3.27e-01 ± 3.07e-02\n",
      "  17. WGAN_WI_OC      - 3.28e-01 ± 1.29e-02\n",
      "  18. WGAN_WI_NO      - 3.33e-01 ± 7.08e-03\n",
      "  19. SGAN_WO_OC      - 3.37e-01 ± 2.93e-02\n",
      "  20. SGAN_WO_OD      - 3.38e-01 ± 3.33e-02\n",
      "  21. SGAN_OR_OD      - 3.48e-01 ± 4.54e-02\n",
      "  22. SGAN_WO_BO      - 3.50e-01 ± 3.13e-02\n",
      "  23. SGAN_OR_BO      - 3.69e-01 ± 3.37e-02\n",
      "  24. TGAN            - 3.74e-01 ± 2.56e-02\n",
      "  25. WGGP_OR_NO      - 3.90e-01 ± 1.09e-02\n",
      "  26. WGGP_OR_OC      - 3.91e-01 ± 1.54e-02\n",
      "  27. WGAN_WO_NO      - 3.98e-01 ± 1.68e-02\n",
      "  28. WGAN_OR_OD      - 4.03e-01 ± 1.84e-02\n",
      "  29. WGAN_OR_BO      - 4.11e-01 ± 2.87e-02\n",
      "  30. SGAN_OR_NO      - 4.35e-01 ± 2.95e-02\n",
      "  31. WGAN_WO_OC      - 4.41e-01 ± 1.90e-02\n",
      "  32. WGAN_WI_BO      - 4.46e-01 ± 1.63e-02\n",
      "  33. WGAN_WI_OD      - 4.47e-01 ± 9.35e-03\n",
      "  34. SGAN_OR_OC      - 4.51e-01 ± 2.38e-02\n",
      "  35. WGAN_OR_NO      - 4.59e-01 ± 7.51e-03\n",
      "  36. WGAN_OR_OC      - 4.61e-01 ± 1.51e-02\n",
      "  37. WGAN_WO_OD      - 4.99e-01 ± 1.50e-02\n",
      "  38. CTGAN           - 5.39e-01 ± 2.50e-02\n",
      "  39. WGAN_WO_BO      - 5.49e-01 ± 2.39e-02\n",
      "  40. TVAE            - 6.82e-01 ± 3.75e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats:\n",
    "    print('Ranking on all coupled combinations based on {}:'.format(s.upper()))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<15} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats per trouple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 3):\n",
    "    combs.append(k[0] + '::' + k[1] + '::' + k[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous pickel file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'trouple_combinations.pickle'.format(dataset)\n",
    "\n",
    "all_stats = {}\n",
    "\n",
    "try:\n",
    "    all_stats = pickle.load(open(filepath + filename, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mCTGAN\u001b[0m (1/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_BO\u001b[0m (2/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_NO\u001b[0m (3/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_OC\u001b[0m (4/39) already exists!\n",
      "Results for model \u001b[1mSGAN_OR_OD\u001b[0m (5/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_BO\u001b[0m (6/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_NO\u001b[0m (7/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_OC\u001b[0m (8/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WI_OD\u001b[0m (9/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_BO\u001b[0m (10/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_NO\u001b[0m (11/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_OC\u001b[0m (12/39) already exists!\n",
      "Results for model \u001b[1mSGAN_WO_OD\u001b[0m (13/39) already exists!\n",
      "Results for model \u001b[1mTGAN\u001b[0m (14/39) already exists!\n",
      "Results for model \u001b[1mTVAE\u001b[0m (15/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_BO\u001b[0m (16/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_NO\u001b[0m (17/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_OC\u001b[0m (18/39) already exists!\n",
      "Results for model \u001b[1mWGAN_OR_OD\u001b[0m (19/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_BO\u001b[0m (20/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_NO\u001b[0m (21/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_OC\u001b[0m (22/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WI_OD\u001b[0m (23/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_BO\u001b[0m (24/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_NO\u001b[0m (25/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_OC\u001b[0m (26/39) already exists!\n",
      "Results for model \u001b[1mWGAN_WO_OD\u001b[0m (27/39) already exists!\n",
      "Preparing stats for model \u001b[1mWGGP_OR_BO\u001b[0m (28/39)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_NO\u001b[0m (29/39)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_OC\u001b[0m (30/39)\n",
      "Preparing stats for model \u001b[1mWGGP_OR_OD\u001b[0m (31/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_BO\u001b[0m (32/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_NO\u001b[0m (33/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_OC\u001b[0m (34/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WI_OD\u001b[0m (35/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_BO\u001b[0m (36/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_NO\u001b[0m (37/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_OC\u001b[0m (38/39)\n",
      "Preparing stats for model \u001b[1mWGGP_WO_OD\u001b[0m (39/39)\n",
      "\u001b[1mFINISHED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Go through each model\n",
    "for i, m in enumerate(models):\n",
    "\n",
    "    if m in all_stats:\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists!\".format(m, i+1, len(models)))\n",
    "    else:\n",
    "        print(\"Preparing stats for model \\033[1m{}\\033[0m ({}/{})\".format(m, i+1, len(models)))\n",
    "\n",
    "        all_stats[m] = {}\n",
    "\n",
    "        for c in combs:\n",
    "            all_stats[m][c] = {}\n",
    "            for s in stats_str:\n",
    "                all_stats[m][c][s] = []\n",
    "\n",
    "        # Load all dataframes for current model\n",
    "        dfs = [pd.read_csv(input_folder + f) for f in files_[m]]\n",
    "\n",
    "        # Go through all dataframes generated for each model\n",
    "        for df in dfs:\n",
    "\n",
    "            # Discretize continuous columns\n",
    "            for c in continuous_cols:\n",
    "                df[c] = pd.cut(df[c], bins=bins_cont[c])\n",
    "\n",
    "            # Go through each columns\n",
    "            for c in combs:\n",
    "\n",
    "                agg_vars = c.split('::')\n",
    "\n",
    "                real = df_orig.copy()\n",
    "                real['count'] = 1\n",
    "                real = real.groupby(agg_vars, observed=True).count()\n",
    "                real /= len(df_orig)\n",
    "\n",
    "                synth = df.copy()\n",
    "                synth['count'] = 1\n",
    "                synth = synth.groupby(agg_vars, observed=True).count()\n",
    "                synth /= len(df)\n",
    "\n",
    "                real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "                real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "                sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "                for s in sts:\n",
    "                    all_stats[m][c][s].append(sts[s])\n",
    "    \n",
    "        pickle.dump(all_stats, open(filepath + filename, 'wb'))\n",
    "\n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in all_stats:\n",
    "    stats_orig = {}\n",
    "\n",
    "    for c in combs:\n",
    "        stats_orig[c] = {}\n",
    "        for s in stats_str:\n",
    "            stats_orig[c][s] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "\n",
    "        # Go through each columns\n",
    "        for c in combs:\n",
    "\n",
    "            agg_vars = c.split('::')\n",
    "\n",
    "            real = train.copy()\n",
    "            real['count'] = 1\n",
    "            real = real.groupby(agg_vars, observed=True).count()\n",
    "            real /= len(df_orig)\n",
    "\n",
    "            synth = test.copy()\n",
    "            synth['count'] = 1\n",
    "            synth = synth.groupby(agg_vars, observed=True).count()\n",
    "            synth /= len(df)\n",
    "\n",
    "            real_and_sampled = pd.merge(real, synth, suffixes=['_real', '_sampled'], on=agg_vars, how='outer', indicator=True)\n",
    "            real_and_sampled = real_and_sampled[['count_real', 'count_sampled']].fillna(0)\n",
    "\n",
    "            sts = compute_stats(real_and_sampled['count_real'], real_and_sampled['count_sampled'])\n",
    "\n",
    "            for s in sts:\n",
    "                stats_orig[c][s].append(sts[s])\n",
    "                \n",
    "    all_stats[orig_str] = stats_orig\n",
    "    \n",
    "    pickle.dump(all_stats, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in all_stats.keys():\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(all_stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in all_stats.keys():\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all triple combinations based on SRMSE:\n",
      "   1. random-original - 3.91e-01 ± 8.54e-03\n",
      "   2. WGGP_WI_NO      - 4.50e-01 ± 2.00e-02\n",
      "   3. WGGP_WI_OC      - 4.61e-01 ± 2.51e-02\n",
      "   4. WGGP_WI_OD      - 5.33e-01 ± 2.59e-02\n",
      "   5. WGGP_WO_OC      - 5.36e-01 ± 2.82e-02\n",
      "   6. WGGP_WO_NO      - 5.37e-01 ± 3.14e-02\n",
      "   7. WGGP_WI_BO      - 5.39e-01 ± 3.11e-02\n",
      "   8. WGGP_OR_OD      - 5.42e-01 ± 3.12e-02\n",
      "   9. WGGP_OR_BO      - 5.44e-01 ± 2.79e-02\n",
      "  10. WGGP_WO_BO      - 5.49e-01 ± 3.32e-02\n",
      "  11. WGGP_WO_OD      - 5.50e-01 ± 3.14e-02\n",
      "  12. SGAN_WI_OD      - 5.55e-01 ± 3.48e-02\n",
      "  13. SGAN_WI_OC      - 5.60e-01 ± 2.29e-02\n",
      "  14. SGAN_WI_NO      - 5.60e-01 ± 3.52e-02\n",
      "  15. SGAN_WI_BO      - 5.62e-01 ± 2.76e-02\n",
      "  16. WGAN_WI_OC      - 6.22e-01 ± 1.67e-02\n",
      "  17. SGAN_WO_NO      - 6.32e-01 ± 5.95e-02\n",
      "  18. SGAN_WO_OD      - 6.33e-01 ± 6.64e-02\n",
      "  19. SGAN_WO_OC      - 6.37e-01 ± 5.65e-02\n",
      "  20. WGAN_WI_NO      - 6.47e-01 ± 8.48e-03\n",
      "  21. SGAN_WO_BO      - 6.48e-01 ± 6.56e-02\n",
      "  22. SGAN_OR_OD      - 6.64e-01 ± 7.62e-02\n",
      "  23. TGAN            - 6.89e-01 ± 4.55e-02\n",
      "  24. SGAN_OR_BO      - 6.96e-01 ± 5.90e-02\n",
      "  25. WGGP_OR_NO      - 7.31e-01 ± 3.20e-02\n",
      "  26. WGGP_OR_OC      - 7.31e-01 ± 2.63e-02\n",
      "  27. WGAN_OR_OD      - 7.42e-01 ± 1.71e-02\n",
      "  28. WGAN_OR_BO      - 7.47e-01 ± 4.60e-02\n",
      "  29. WGAN_WO_NO      - 7.73e-01 ± 5.74e-02\n",
      "  30. WGAN_WI_OD      - 8.02e-01 ± 1.44e-02\n",
      "  31. WGAN_WO_OC      - 8.08e-01 ± 3.36e-02\n",
      "  32. WGAN_WI_BO      - 8.09e-01 ± 2.44e-02\n",
      "  33. SGAN_OR_NO      - 8.34e-01 ± 4.48e-02\n",
      "  34. SGAN_OR_OC      - 8.38e-01 ± 3.03e-02\n",
      "  35. WGAN_OR_OC      - 8.59e-01 ± 2.03e-02\n",
      "  36. WGAN_WO_OD      - 8.89e-01 ± 4.28e-02\n",
      "  37. WGAN_OR_NO      - 9.06e-01 ± 4.27e-02\n",
      "  38. WGAN_WO_BO      - 9.66e-01 ± 4.35e-02\n",
      "  39. CTGAN           - 9.67e-01 ± 3.59e-02\n",
      "  40. TVAE            - 1.21e+00 ± 7.04e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats_str:\n",
    "    print('Ranking on all triple combinations based on {}:'.format(s.upper()))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<15} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
