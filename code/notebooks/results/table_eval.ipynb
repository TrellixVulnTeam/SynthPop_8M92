{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.table_evaluator import load_data, TableEvaluator\n",
    "# From https://github.com/Baukebrenninkmeijer/table-evaluator\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "# For the Python notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_DATGAN(name):\n",
    "    if any(x in name for x in ['TGAN', 'CTGAN', 'TVAE', 'FULL', 'TRANSRED', 'LINEAR', 'NOLINKS', 'PREDICTION']):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def check_low_appearing_vars(df):\n",
    "    \n",
    "    for c in df.columns:\n",
    "        val = df[c].value_counts()\n",
    "        if len(val) < 20:\n",
    "            val = val/len(df)\n",
    "            if any(val < 0.01) and c != 'choice':\n",
    "                print('Variable {}: '.format(c))\n",
    "                for idx, v in zip(val.index, val):\n",
    "                    if v < 0.01:\n",
    "                        print('  {} - {:.2f}% ({:d})'.format(idx, 100*v, int(v*len(df))))\n",
    "                print()\n",
    "                \n",
    "def replace_low_appearing_values(df, dataset):\n",
    "    \n",
    "    if 'Chicago' in dataset:\n",
    "        dct_ = {}\n",
    "        for i in df['hh_vehicles'].unique():\n",
    "            if i >= 5:\n",
    "                dct_[i] = '5+'\n",
    "            else:\n",
    "                dct_[i] = str(i)        \n",
    "        df['hh_vehicles'].replace(dct_, inplace=True)\n",
    "        \n",
    "        dct_ = {}\n",
    "        for i in df['hh_size'].unique():\n",
    "            if i >= 6:\n",
    "                dct_[i] = '6+'\n",
    "            else:\n",
    "                dct_[i] = str(i)        \n",
    "        df['hh_size'].replace(dct_, inplace=True)\n",
    "        \n",
    "        dct_ = {}\n",
    "        for i in df['hh_bikes'].unique():\n",
    "            if i >= 6:\n",
    "                dct_[i] = '6+'\n",
    "            else:\n",
    "                dct_[i] = str(i)        \n",
    "        df['hh_bikes'].replace(dct_, inplace=True)       \n",
    "\n",
    "    elif 'LPMC' in dataset:\n",
    "        dct_ = {}\n",
    "        for i in df['pt_n_interchanges'].unique():\n",
    "            if i >= 2:\n",
    "                dct_[i] = '2+'\n",
    "            else:\n",
    "                dct_[i] = str(i)        \n",
    "        df['pt_n_interchanges'].replace(dct_, inplace=True) \n",
    "        \n",
    "        dct_ = {\n",
    "            'Diesel_LGV': 'LGV',\n",
    "            'Petrol_LGV': 'LGV',\n",
    "            'Hybrid_Car': 'Average_Car'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Chicago_DAG'\n",
    "orig_str = 'random-original'\n",
    "input_folder = '../synth_data/{}/'.format(dataset)\n",
    "n_models = 5\n",
    "n_data = 5\n",
    "\n",
    "# Models for testing all DATGANS\n",
    "models = ['CTGAN', 'TGAN', 'TVAE']\n",
    "\n",
    "for i in ['WGAN', 'SGAN', 'WGGP']:\n",
    "    for j in ['WI', 'OR', 'WO']:\n",
    "        for k in ['NO', 'BO', 'OD', 'OC']:\n",
    "            models.append('{}_{}_{}'.format(i,j,k))\n",
    "            \n",
    "# Models for testing different DAGs\n",
    "if 'DAG' in dataset:\n",
    "    models = ['FULL', 'TRANSRED', 'LINEAR', 'NOLINKS', 'PREDICTION']\n",
    "\n",
    "models.sort()\n",
    "\n",
    "files_ = {}\n",
    "\n",
    "for m in models:\n",
    "    tmp = []\n",
    "    if is_a_DATGAN(m):\n",
    "        spl = m.split('_')\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append(input_folder + '{}_{}_{:0>2}_{}_{:0>2}.csv'.format(spl[0], spl[1], i+1,  spl[2], j+1))\n",
    "    else:\n",
    "        for i in range(n_models):\n",
    "            for j in range(n_data):\n",
    "                tmp.append(input_folder + '{}_{:0>2}_{:0>2}.csv'.format(m, i+1, j+1))\n",
    "    files_[m] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Chicago' in dataset:\n",
    "    continuous_columns = [\"distance\", \"age\", \"departure_time\"]\n",
    "elif 'LPMC' in dataset:\n",
    "    continuous_columns = ['start_time_linear', 'age', 'distance', 'dur_walking', 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', 'dur_pt_int', 'dur_driving', 'cost_transit', 'cost_driving_fuel', 'driving_traffic_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('../data/' + dataset.split('_')[0] + '/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_low_appearing_values(df_orig, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_low_appearing_vars(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(set(df_orig.columns) - set(continuous_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found previous picle file, using that\n"
     ]
    }
   ],
   "source": [
    "filepath = './notebooks/results/{}/'.format(dataset)\n",
    "filename = 'te_results.pickle'\n",
    "te_results = {}\n",
    "n = 7000\n",
    "\n",
    "if 'Chicago' in dataset:\n",
    "    target_col = 'choice'\n",
    "elif 'LPMC' in dataset:\n",
    "    target_col = 'mode_choice'\n",
    "\n",
    "try:\n",
    "    te_results = pickle.load(open(f'{filepath}{filename}','rb'))\n",
    "    print('Found previous picle file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for model \u001b[1mFULL\u001b[0m (1/5) already exists\n",
      "Results for model \u001b[1mLINEAR\u001b[0m (2/5) already exists\n",
      "Results for model \u001b[1mNOLINKS\u001b[0m (3/5) already exists\n",
      "Results for model \u001b[1mPREDICTION\u001b[0m (4/5) already exists\n",
      "Results for model \u001b[1mTRANSRED\u001b[0m (5/5) already exists\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    \n",
    "    if model in te_results.keys():\n",
    "        print(\"Results for model \\033[1m{}\\033[0m ({}/{}) already exists\".format(model, i+1, len(models)))\n",
    "    else:\n",
    "        print(\"Getting results for model \\033[1m{}\\033[0m ({}/{})\".format(model, i+1, len(models)))\n",
    "        te_results[model] = []\n",
    "        \n",
    "    n_files_done = len(te_results[model])\n",
    "    \n",
    "    for j, f in enumerate(files_[model][n_files_done:]):\n",
    "        print(\"  Processing file {}/{}\".format(j+1+n_files_done, len(files_[model])) + \" \"*20)\n",
    "\n",
    "        tmp_df = pd.read_csv(f)\n",
    "        replace_low_appearing_values(tmp_df, dataset)\n",
    "        \n",
    "        te = TableEvaluator(tmp_df, df_orig, cat_cols, verbose=False)\n",
    "        \n",
    "        res = te.evaluate(target_col=target_col, kfold=True)\n",
    "        \n",
    "        te_results[model].append(res.content.to_dict()['result'])\n",
    "        \n",
    "        pickle.dump(te_results, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if orig_str not in te_results:\n",
    "    te_results[orig_str] = []\n",
    "\n",
    "    for i in range(n_models*n_data):\n",
    "        print(\"Processing random dataset {}/{}\".format(i+1, len(files_[model])) + \" \"*20)\n",
    "\n",
    "        train = df_orig.sample(int(len(df_orig) * 0.5))\n",
    "        train.index = range(len(train))\n",
    "        test = df_orig[~df_orig.index.isin(train.index)]\n",
    "        test.index = range(len(test))\n",
    "        \n",
    "        te = TableEvaluator(test, train, cat_cols, verbose=False)\n",
    "        \n",
    "        res = te.evaluate(target_col=target_col, kfold=True)\n",
    "        \n",
    "        te_results[orig_str].append(res.content.to_dict()['result'])\n",
    "        \n",
    "        pickle.dump(te_results, open(filepath + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = te_results[orig_str][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for k in keys:\n",
    "    res[k] = {}\n",
    "            \n",
    "    for m in te_results.keys():\n",
    "        \n",
    "        tmp = []\n",
    "        \n",
    "        for i in range(n_models*n_data):\n",
    "            tmp.append(te_results[m][i][k])\n",
    "            \n",
    "        res[k][m] = {\n",
    "            'mean': np.mean(tmp),\n",
    "            'std': np.std(tmp)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on \"Basic statistics\":\n",
      "   1. random-original - 1.00e+00 ± 0.00e+00\n",
      "   2. LINEAR          - 9.97e-01 ± 6.89e-03\n",
      "   3. FULL            - 9.93e-01 ± 9.06e-03\n",
      "   4. NOLINKS         - 9.86e-01 ± 9.28e-03\n",
      "   5. PREDICTION      - 9.85e-01 ± 8.92e-03\n",
      "   6. TRANSRED        - 9.82e-01 ± 8.42e-03\n",
      "\n",
      "Ranking on \"Correlation column correlations\":\n",
      "   1. random-original - 9.96e-01 ± 0.00e+00\n",
      "   2. FULL            - 9.85e-01 ± 1.68e-03\n",
      "   3. TRANSRED        - 9.84e-01 ± 3.17e-03\n",
      "   4. LINEAR          - 9.77e-01 ± 1.86e-03\n",
      "   5. NOLINKS         - 4.32e-01 ± 5.20e-02\n",
      "   6. PREDICTION      - 3.88e-01 ± 3.97e-02\n",
      "\n",
      "Ranking on \"Mean Correlation between fake and real columns\":\n",
      "   1. PREDICTION      - 9.52e-01 ± 3.75e-03\n",
      "   2. NOLINKS         - 9.50e-01 ± 4.89e-03\n",
      "   3. TRANSRED        - 9.35e-01 ± 7.42e-03\n",
      "   4. LINEAR          - 9.33e-01 ± 6.78e-03\n",
      "   5. FULL            - 9.23e-01 ± 9.96e-03\n",
      "   6. random-original - 9.17e-01 ± 1.11e-16\n",
      "\n",
      "Ranking on \"1 - MAPE Estimator results\":\n",
      "   1. TRANSRED        - 9.71e-01 ± 3.27e-03\n",
      "   2. FULL            - 9.69e-01 ± 3.18e-03\n",
      "   3. LINEAR          - 9.62e-01 ± 4.69e-03\n",
      "   4. random-original - 9.15e-01 ± 1.11e-16\n",
      "   5. PREDICTION      - 8.90e-01 ± 9.05e-03\n",
      "   6. NOLINKS         - 7.72e-01 ± 6.21e-03\n",
      "\n",
      "Ranking on \"Similarity Score\":\n",
      "   1. TRANSRED        - 9.68e-01 ± 3.73e-03\n",
      "   2. LINEAR          - 9.67e-01 ± 1.86e-03\n",
      "   3. FULL            - 9.67e-01 ± 3.83e-03\n",
      "   4. random-original - 9.57e-01 ± 1.11e-16\n",
      "   5. PREDICTION      - 8.04e-01 ± 9.85e-03\n",
      "   6. NOLINKS         - 7.85e-01 ± 1.40e-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in keys:\n",
    "    print('Ranking on \"{}\":'.format(s))\n",
    "\n",
    "    sorted_dct = {k: v for k, v in sorted(res[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<15} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
