{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For the Python notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Chicago'\n",
    "df_orig = pd.read_csv('../../../data/' + dataset.split('_')[0] + '/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Chicago' in dataset:\n",
    "    continuous_cols = ['distance', 'age', 'departure_time']\n",
    "elif 'LPMC' in dataset:\n",
    "    continuous_cols = ['start_time_linear', 'age', 'distance', 'dur_walking', 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', 'dur_pt_int', 'dur_driving', 'cost_transit', 'cost_driving_fuel', 'driving_traffic_percent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_str = ['mae', 'rmse', 'r2', 'srmse', 'corr']\n",
    "stats_tex = {\n",
    "    'mae': '\\\\textbf{MAE}',\n",
    "    'rmse': '\\\\textbf{RMSE}',\n",
    "    'srmse': '\\\\textbf{SRMSE}', \n",
    "    'r2': '$\\\\bf R^2$',\n",
    "    'corr': '$\\\\bf \\\\rho_{\\\\text{Pearson} }$'\n",
    "}\n",
    "\n",
    "dataset_name = {\n",
    "    'Chicago': 'CMAP',\n",
    "    'LPMC': 'LPMC'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 5\n",
    "n_data = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_name(name):\n",
    "    dct = {\n",
    "        'WO': 'NO',\n",
    "        'OR': 'OS',\n",
    "        'WI': 'TS',\n",
    "        'OC': 'CO',\n",
    "        'OD': 'CA',\n",
    "    }\n",
    "    \n",
    "    for k,v in dct.items():\n",
    "        name = name.replace(k, v)\n",
    "    \n",
    "    return name\n",
    "\n",
    "def to_TeX(num, val=1):\n",
    "    num = \"{{0:.{}e}}\".format(val).format(num)\n",
    "    mantissa, exponent = num.split('e')\n",
    "    exponent = int(exponent)\n",
    "    return \"{0}e{{{1}}}\".format(mantissa, exponent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = []\n",
    "\n",
    "for l in ['SGAN', 'WGAN', 'WGGP']:\n",
    "    for ls in ['WO', 'OR', 'WI']:\n",
    "        for s in ['BO', 'OC', 'OD', 'NO']:\n",
    "            order.append('{}_{}_{}'.format(l, ls, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats - first level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pickle.load(open('./{}/single_columns.pickle'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    res[test] = {}\n",
    "    \n",
    "    if test == 'all':\n",
    "        cols = df_orig.columns\n",
    "    elif test == 'cont':\n",
    "        cols = continuous_cols\n",
    "    elif test == 'cat':\n",
    "        cols = set(df_orig.columns) - set(continuous_cols)\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[test][s] = {}\n",
    "\n",
    "    for m in order:\n",
    "\n",
    "        for s in stats_str:\n",
    "            res[test][s][m] = []\n",
    "\n",
    "            for i in range(n_models*n_data):\n",
    "                tmp = []\n",
    "\n",
    "                for c in cols:\n",
    "                    tmp.append(stats[m][c][s][i])\n",
    "\n",
    "                res[test][s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    avg[test] = {}\n",
    "\n",
    "    for s in stats_str:\n",
    "        avg[test][s] = {}\n",
    "\n",
    "        for m in order:\n",
    "            avg[test][s][m] = {\n",
    "                'mean': np.mean(res[test][s][m]),\n",
    "                'std': np.std(res[test][s][m])\n",
    "            }\n",
    "            \n",
    "        if s in ['r2', 'corr']:\n",
    "            sorted_list = [k for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])[::-1]]\n",
    "        else:\n",
    "            sorted_list = [k for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])]\n",
    "\n",
    "        for i, m in enumerate(sorted_list):\n",
    "            avg[test][s][m]['rank'] = i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for test in ['all', 'cont', 'cat']:\n",
    "    table = ''\n",
    "    table += '\\\\begin{table}[H]\\n'\n",
    "    table += '\\t\\\\centering\\n'\n",
    "    \n",
    "    test_str = {\n",
    "        'cont': 'continuous',\n",
    "        'cat': 'categorical',\n",
    "        'all': 'all'\n",
    "    }\n",
    "    \n",
    "    table += '\\t\\\\caption{{Results for the statistics on the first level ({} columns) for the {} dataset}}\\n'.format(test_str[test], dataset_name[dataset])\n",
    "    table += '\\t\\\\label{{tab:first_{}_{}}}\\n'.format(test, dataset)\n",
    "    table += '\\t\\\\begin{{tabularx}}{{\\\\textwidth}}{{l|{}}}\\n'.format('|cC'*len(stats_str)+'||c')\n",
    "    \n",
    "    str_ = ''\n",
    "    for i, s in enumerate(stats_str):\n",
    "        if i == len(stats_str)-1:\n",
    "            str_ += '& \\multicolumn{{2}}{{c||}}{{{}}} '.format(stats_tex[s])\n",
    "        else:    \n",
    "            str_ += '& \\multicolumn{{2}}{{c|}}{{{}}} '.format(stats_tex[s])\n",
    "        \n",
    "    \n",
    "    str_ += '& \\\\multicolumn{1}{c}{\\\\textbf{rank}} '\n",
    "    \n",
    "    table += '\\t\\\\multicolumn{{1}}{{c||}}{{\\\\textbf{{Name}}}} {} \\\\\\\\ \\\\midrule[1.5pt]\\n'.format(str_)\n",
    "    for i, m in enumerate(order):\n",
    "        table += '\\t\\t\\\\texttt{{{}}} & '.format(change_name(m).replace('_', '\\_'))\n",
    "        tmp_rank = []\n",
    "        for j, s in enumerate(stats_str):\n",
    "            tmp = avg[test][s][m]\n",
    "            tmp_rank.append(tmp['rank'])\n",
    "            \n",
    "            if tmp['rank'] <=10:\n",
    "                suff = '\\\\bf'\n",
    "            else:\n",
    "                suff = ''\n",
    "                \n",
    "            val = round(100*(tmp['rank']-1)/(len(order)-1))\n",
    "                \n",
    "            table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$'.format(val, suff, tmp['rank'], val, suff, tmp['mean'])\n",
    "            \n",
    "            table += ' & '\n",
    "            \n",
    "        # Avg rank\n",
    "        avg_rank = np.mean(tmp_rank)\n",
    "\n",
    "        if avg_rank <=10:\n",
    "            suff = '\\\\bf'\n",
    "        else:\n",
    "            suff = ''\n",
    "\n",
    "        val = round(100*(avg_rank-1)/(len(order)-1))\n",
    "\n",
    "        table += '\\\\cellcolor{{Gray!{}}}${} {:.1f}$  \\\\\\\\'.format(val, suff, avg_rank)\n",
    "            \n",
    "        if (i+1)%12 == 0 and (i+1) < len(order):\n",
    "            table += ' \\\\hline'\n",
    "\n",
    "        table += '\\n'\n",
    "        \n",
    "    table += '\\t\\\\end{tabularx}\\n'\n",
    "    table += '\\\\end{table}\\n'\n",
    "    \n",
    "    with open('./tables/first_{}_{}.tex'.format(test, dataset), 'w') as infile:\n",
    "        infile.write(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics - second level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pickle.load(open('./{}/couple_combinations.pickle'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 2):\n",
    "    combs.append(k[0] + '::' + k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in order:\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in order:\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_list = [k for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]]\n",
    "    else:\n",
    "        sorted_list = [k for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])]\n",
    "\n",
    "    for i, m in enumerate(sorted_list):\n",
    "        avg[s][m]['rank'] = i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = ''\n",
    "table += '\\\\begin{table}[H]\\n'\n",
    "table += '\\t\\\\centering\\n'\n",
    "table += '\\t\\\\caption{{Results for the statistics on the second level for the {} dataset}}\\n'.format(dataset_name[dataset])\n",
    "table += '\\t\\\\label{{tab:second_{}}}\\n'.format(dataset)\n",
    "\n",
    "table += '\\t\\\\begin{{tabularx}}{{\\\\textwidth}}{{l|{}}}\\n'.format('|cC'*len(stats_str)+'||c')\n",
    "\n",
    "str_ = ''\n",
    "for i, s in enumerate(stats_str):\n",
    "    if i == len(stats_str)-1:\n",
    "        str_ += '& \\multicolumn{{2}}{{c||}}{{{}}} '.format(stats_tex[s])\n",
    "    else:    \n",
    "        str_ += '& \\multicolumn{{2}}{{c|}}{{{}}} '.format(stats_tex[s])\n",
    "\n",
    "\n",
    "str_ += '& \\\\multicolumn{1}{c}{\\\\textbf{rank}} '\n",
    "\n",
    "table += '\\t\\\\multicolumn{{1}}{{c||}}{{\\\\textbf{{Name}}}} {} \\\\\\\\ \\\\midrule[1.5pt]\\n'.format(str_)\n",
    "\n",
    "for i, m in enumerate(order):\n",
    "    table += '\\t\\t\\\\texttt{{{}}} & '.format(change_name(m).replace('_', '\\_'))\n",
    "    tmp_rank = []\n",
    "    for j, s in enumerate(stats_str):\n",
    "        tmp = avg[s][m]\n",
    "        tmp_rank.append(tmp['rank'])\n",
    "\n",
    "        if tmp['rank'] <=10:\n",
    "            suff = '\\\\bf'\n",
    "        else:\n",
    "            suff = ''\n",
    "\n",
    "        val = round(100*(tmp['rank']-1)/(len(order)-1))\n",
    "\n",
    "        table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$ & '.format(val, suff, tmp['rank'], val, suff, tmp['mean'])\n",
    "            \n",
    "    # Avg rank\n",
    "    avg_rank = np.mean(tmp_rank)\n",
    "\n",
    "    if avg_rank <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(avg_rank-1)/(len(order)-1))\n",
    "\n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:.1f}$  \\\\\\\\'.format(val, suff, avg_rank)\n",
    "\n",
    "    if (i+1)%12 == 0 and (i+1) < len(order):\n",
    "        table += ' \\\\hline'\n",
    "\n",
    "    table += '\\n'\n",
    "\n",
    "table += '\\t\\\\end{tabularx}\\n'\n",
    "table += '\\\\end{table}\\n'\n",
    "\n",
    "with open('./tables/second_{}.tex'.format(dataset), 'w') as infile:\n",
    "    infile.write(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats - third level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pickle.load(open('./{}/trouple_combinations.pickle'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 3):\n",
    "    combs.append(k[0] + '::' + k[1] + '::' + k[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in order:\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in order:\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_list = [k for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]]\n",
    "    else:\n",
    "        sorted_list = [k for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])]\n",
    "\n",
    "    for i, m in enumerate(sorted_list):\n",
    "        avg[s][m]['rank'] = i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table = ''\n",
    "table += '\\\\begin{table}[H]\\n'\n",
    "table += '\\t\\\\centering\\n'\n",
    "table += '\\t\\\\caption{{Results for the statistics on the third level for the {} dataset}}\\n'.format(dataset_name[dataset])\n",
    "table += '\\t\\\\label{{tab:third_{}}}\\n'.format(dataset)\n",
    "table += '\\t\\\\begin{{tabularx}}{{\\\\textwidth}}{{l|{}}}\\n'.format('|cC'*len(stats_str)+'||c')\n",
    "\n",
    "str_ = ''\n",
    "for i, s in enumerate(stats_str):\n",
    "    if i == len(stats_str)-1:\n",
    "        str_ += '& \\multicolumn{{2}}{{c||}}{{{}}} '.format(stats_tex[s])\n",
    "    else:    \n",
    "        str_ += '& \\multicolumn{{2}}{{c|}}{{{}}} '.format(stats_tex[s])\n",
    "\n",
    "\n",
    "str_ += '& \\\\multicolumn{1}{c}{\\\\textbf{rank}} '\n",
    "\n",
    "table += '\\t\\\\multicolumn{{1}}{{c||}}{{\\\\textbf{{Name}}}} {} \\\\\\\\ \\\\midrule[1.5pt]\\n'.format(str_)\n",
    "\n",
    "for i, m in enumerate(order):\n",
    "    table += '\\t\\t\\\\texttt{{{}}} & '.format(change_name(m).replace('_', '\\_'))\n",
    "    tmp_rank = []\n",
    "    for j, s in enumerate(stats_str):\n",
    "        tmp = avg[s][m]\n",
    "        tmp_rank.append(tmp['rank'])\n",
    "\n",
    "        if tmp['rank'] <=10:\n",
    "            suff = '\\\\bf'\n",
    "        else:\n",
    "            suff = ''\n",
    "\n",
    "        val = round(100*(tmp['rank']-1)/(len(order)-1))\n",
    "\n",
    "        table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$ & '.format(val, suff, tmp['rank'], val, suff, tmp['mean'])\n",
    "            \n",
    "    # Avg rank\n",
    "    avg_rank = np.mean(tmp_rank)\n",
    "\n",
    "    if avg_rank <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(avg_rank-1)/(len(order)-1))\n",
    "\n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:.1f}$  \\\\\\\\'.format(val, suff, avg_rank)\n",
    "\n",
    "    if (i+1)%12 == 0 and (i+1) < len(order):\n",
    "        table += ' \\\\hline'\n",
    "\n",
    "    table += '\\n'\n",
    "\n",
    "table += '\\t\\\\end{tabularx}\\n'\n",
    "table += '\\\\end{table}\\n'\n",
    "\n",
    "\n",
    "with open('./tables/third_{}.tex'.format(dataset), 'w') as infile:\n",
    "    infile.write(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML efficacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_modelscores = pickle.load(open('./{}/cv_result_ml.pickle'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Chicago' in dataset:\n",
    "    cont_cols = ['distance', 'age', 'departure_time']\n",
    "    ord_cols = ['hh_vehicles', 'hh_size', 'hh_bikes', 'hh_income', 'education_level']\n",
    "    cat_cols = [col for col in df_orig.columns if col not in cont_cols + ord_cols]\n",
    "elif 'LPMC' in dataset:\n",
    "    cont_cols = ['start_time_linear', 'age', 'distance', 'dur_walking', \n",
    "                 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', \n",
    "                 'dur_pt_int', 'dur_driving', 'cost_transit', \n",
    "                 'cost_driving_fuel', 'driving_traffic_percent']\n",
    "    ord_cols = ['travel_year', 'travel_month', 'travel_date', \n",
    "                'day_of_week', 'pt_n_interchanges', 'car_ownership']\n",
    "    cat_cols = [col for col in df_orig.columns if col not in cont_cols + ord_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_scores = {col: cv_modelscores['original'][0][col]['test_log_loss'] for col in cat_cols + ord_cols}\n",
    "ori_scores.update({col: cv_modelscores['original'][0][col]['test_l2'] for col in cont_cols})\n",
    "\n",
    "internal = {}\n",
    "external = {}\n",
    "external_normalised = {}\n",
    "cont_scores = {}\n",
    "cat_scores = {}\n",
    "\n",
    "for model in order:\n",
    "    \n",
    "    n_tests = len(cv_modelscores[model])\n",
    "    \n",
    "    internal[model] = {}\n",
    "    external[model] = {}\n",
    "    external_normalised[model] = {}\n",
    "    for col in cat_cols + ord_cols:\n",
    "        tmp = [cv_modelscores[model][i][col]['test_log_loss'] for i in range(n_tests)]\n",
    "        internal[model][col] = {'avg': np.mean(tmp), 'std': np.std(tmp)}\n",
    "        \n",
    "        tmp = [cv_modelscores[model][i][col]['original_log_loss'] for i in range(n_tests)]\n",
    "        external[model][col] = {'avg': np.mean(tmp), 'std': np.std(tmp)}\n",
    "        \n",
    "        external_normalised[model][col] = external[model][col]['avg'] - ori_scores[col]\n",
    "\n",
    "        \n",
    "    for col in cont_cols:\n",
    "        tmp = [cv_modelscores[model][i][col]['test_l2'] for i in range(n_tests)]\n",
    "        internal[model][col] = {'avg': np.mean(tmp), 'std': np.std(tmp)}\n",
    "        \n",
    "        tmp = [cv_modelscores[model][i][col]['original_l2'] for i in range(n_tests)]\n",
    "        external[model][col] = {'avg': np.mean(tmp), 'std': np.std(tmp)}\n",
    "        \n",
    "        external_normalised[model][col] = external[model][col]['avg'] - ori_scores[col]\n",
    "    \n",
    "    cont_scores[model] = sum([external[model][col]['avg']/ori_scores[col] for col in cont_cols])\n",
    "    cat_scores[model] = sum([external[model][col]['avg']-ori_scores[col] for col in cat_cols + ord_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sorted = sorted(cat_scores.items(), key=lambda item: item[1])\n",
    "cont_sorted = sorted(cont_scores.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = ''\n",
    "table += '\\\\begin{table}[H]\\n'\n",
    "table += '\\t\\\\centering\\n'\n",
    "table += '\\t\\\\caption{{Results for the Machine Learning efficacy for the {} dataset}}\\n'.format(dataset_name[dataset])\n",
    "table += '\\t\\\\label{{tab:ml_efficacy_{}}}\\n'.format(dataset)\n",
    "table += '\\t\\\\begin{{tabularx}}{{\\\\textwidth}}{{l|{}}}\\n'.format('|CC'*2+'||C')\n",
    "\n",
    "str_ = ''\n",
    "for i, s in enumerate(['Continuous', 'Categorical']):\n",
    "    str_ += '& \\multicolumn{{2}}{{c{}}}{{\\\\textbf{{{}}}}} '.format((i+1)*'|', s)\n",
    "    \n",
    "str_ += '& \\\\multicolumn{1}{c}{\\\\textbf{rank}} '\n",
    "\n",
    "table += '\\t\\\\multicolumn{{1}}{{c||}}{{\\\\textbf{{Name}}}} {} \\\\\\\\ \\\\midrule[1.5pt]\\n'.format(str_)\n",
    "\n",
    "for i, m in enumerate(order):\n",
    "    table += '\\t\\t\\\\texttt{{{}}} & '.format(change_name(m).replace('_', '\\_'))\n",
    "    # continuous\n",
    "    rank_cont = [x+1 for x, y in enumerate(cont_sorted) if y[0] == m][0]\n",
    "    value = cont_sorted[rank_cont-1][1]\n",
    "\n",
    "    if rank_cont <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(rank_cont-1)/(len(order)-1))\n",
    "\n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$ & '.format(val, suff, rank_cont, val, suff, value)\n",
    "\n",
    "    # categorical\n",
    "    rank_cat = [x+1 for x, y in enumerate(cat_sorted) if y[0] == m][0]\n",
    "    value = cat_sorted[rank_cat-1][1]\n",
    "\n",
    "    if rank_cat <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(rank_cat-1)/(len(order)-1))\n",
    "\n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$ &'.format(val, suff, rank_cat, val, suff, value)\n",
    "    \n",
    "    # Avg rank\n",
    "    avg_rank = (rank_cont + rank_cat)/2\n",
    "    \n",
    "    if avg_rank <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(avg_rank-1)/(len(order)-1))\n",
    "    \n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:.1f}$  \\\\\\\\'.format(val, suff, avg_rank)\n",
    "    \n",
    "    if (i+1) < len(order) and (i+1)%12 == 0:\n",
    "        table += ' \\\\hline'\n",
    "\n",
    "    table += '\\n'\n",
    "\n",
    "table += '\\t\\\\end{tabularx}\\n'\n",
    "table += '\\\\end{table}\\n'\n",
    "\n",
    "with open('./tables/ml_efficacy_{}.tex'.format(dataset), 'w') as infile:\n",
    "    infile.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
