{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For the Python notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Chicago'\n",
    "df_orig = pd.read_csv('../../../data/' + dataset.split('_')[0] + '/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Chicago' in dataset:\n",
    "    continuous_cols = ['distance', 'age', 'departure_time']\n",
    "elif 'LPMC' in dataset:\n",
    "    continuous_cols = ['start_time_linear', 'age', 'distance', 'dur_walking', 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', 'dur_pt_int', 'dur_driving', 'cost_transit', 'cost_driving_fuel', 'driving_traffic_percent']\n",
    "elif 'adult' in dataset:\n",
    "    continuous_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_str = ['mae', 'rmse', 'r2', 'srmse', 'corr']\n",
    "stats_tex = {\n",
    "    'mae': '\\\\textbf{MAE}',\n",
    "    'rmse': '\\\\textbf{RMSE}',\n",
    "    'srmse': '\\\\textbf{SRMSE}', \n",
    "    'r2': '\\\\boldmath$R^2$',\n",
    "    'corr': '\\\\boldmath$\\\\rho_{\\\\text{Pearson} }$'\n",
    "}\n",
    "\n",
    "dataset_name = {\n",
    "    'Chicago': 'CMAP',\n",
    "    'LPMC': 'LPMC',\n",
    "    'LPMC_half': 'LPMC\\_half',\n",
    "    'adult': 'ADULT'\n",
    "}\n",
    "\n",
    "model_name = {\n",
    "    'CTABGAN': 'CTABGAN',\n",
    "    'CTGAN': 'CTGAN',\n",
    "    'WGGP_WI_NO': 'DATGAN (\\\\texttt{WGGP})',\n",
    "    'WGAN_WI_NO': 'DATGAN (\\\\texttt{WGAN})',\n",
    "    'TGAN': 'TGAN',\n",
    "    'TVAE': 'TVAE',\n",
    "}\n",
    "\n",
    "col_type = {\n",
    "    'all': 'all',\n",
    "    'cont': 'continuous',\n",
    "    'cat': 'categorical'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 5\n",
    "n_data = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_TeX(num, val=1):\n",
    "    num = \"{{0:.{}e}}\".format(val).format(num)\n",
    "    mantissa, exponent = num.split('e')\n",
    "    exponent = int(exponent)\n",
    "    return \"{0}e{{{1}}}\".format(mantissa, exponent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Chicago' in dataset:\n",
    "    order = ['CTABGAN', 'CTGAN', 'WGAN_WI_NO', 'TGAN', 'TVAE']\n",
    "elif 'LPMC' in dataset:\n",
    "    order = ['CTABGAN', 'CTGAN', 'WGGP_WI_NO', 'TGAN', 'TVAE']\n",
    "if 'adult' in dataset:\n",
    "    order = ['CTABGAN', 'CTGAN', 'WGAN_WI_NO', 'WGGP_WI_NO', 'TGAN', 'TVAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ranks = {}\n",
    "\n",
    "for m in order:\n",
    "    all_ranks[m] = {'stats': [], 'ml': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats - first level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pickle.load(open('./{}/single_columns.pickle'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    res[test] = {}\n",
    "    \n",
    "    if test == 'all':\n",
    "        cols = df_orig.columns\n",
    "    elif test == 'cont':\n",
    "        cols = continuous_cols\n",
    "    elif test == 'cat':\n",
    "        cols = set(df_orig.columns) - set(continuous_cols)\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[test][s] = {}\n",
    "\n",
    "    for m in order:\n",
    "\n",
    "        for s in stats_str:\n",
    "            res[test][s][m] = []\n",
    "\n",
    "            for i in range(n_models*n_data):\n",
    "                tmp = []\n",
    "\n",
    "                for c in cols:\n",
    "                    tmp.append(stats[m][c][s][i])\n",
    "\n",
    "                res[test][s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    avg[test] = {}\n",
    "\n",
    "    for s in stats_str:\n",
    "        avg[test][s] = {}\n",
    "\n",
    "        for m in order:\n",
    "            avg[test][s][m] = {\n",
    "                'mean': np.mean(res[test][s][m]),\n",
    "                'std': np.std(res[test][s][m])\n",
    "            }\n",
    "            \n",
    "        if s in ['r2', 'corr']:\n",
    "            sorted_list = [k for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])[::-1]]\n",
    "        else:\n",
    "            sorted_list = [k for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])]\n",
    "\n",
    "        for i, m in enumerate(sorted_list):\n",
    "            avg[test][s][m]['rank'] = i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table = '\\\\begin{{xltabular}}{{\\\\textwidth}}{{l|{}}}\\n'.format('|cC'*len(stats_str)+'||c')\n",
    "\n",
    "str_ = ''\n",
    "for i, s in enumerate(stats_str):\n",
    "    if i == len(stats_str)-1:\n",
    "        str_ += '& \\multicolumn{{2}}{{c||}}{{{}}} '.format(stats_tex[s])\n",
    "    else:    \n",
    "        str_ += '& \\multicolumn{{2}}{{c|}}{{{}}} '.format(stats_tex[s])\n",
    "\n",
    "\n",
    "str_ += '& \\\\multicolumn{1}{c}{\\\\textbf{rank}} '\n",
    "\n",
    "header = '\\\\multicolumn{{1}}{{c||}}{{\\\\textbf{{Name}}}} {} \\\\\\\\ \\\\midrule[1.5pt]\\n'.format(str_)\n",
    "\n",
    "table += '\\\\caption{{\\\\normalsize Results of the statistical assessments between the best DATGAN version and the state-of-the-art models for the {} dataset. Lighter grey tone corresponds to better results compared to darker ones.}}\\n'.format(dataset_name[dataset])\n",
    "table += '\\\\label{{tab:stats_final_{}}}\\\\\\\\\\n\\n'.format(dataset)\n",
    "table += header\n",
    "table += '\\endfirsthead\\n\\n'\n",
    "table += '\\\\multicolumn{' + str(2*len(stats_str)+2) + '}{c}{\\\\tablename\\\\ \\\\thetable{} -- continued from previous page} \\\\\\\\\\n'\n",
    "table += header\n",
    "table += '\\endhead\\n\\n'\n",
    "table += '\\\\hline\\\\multicolumn{' + str(2*len(stats_str)+2) + '}{r}{{\\\\normalsize Continues on next page...}}\\n'\n",
    "table += '\\endfoot\\n\\n'\n",
    "table += '\\endlastfoot\\n\\n'\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "\n",
    "    table += '\\t\\\\hline \\\\multicolumn{{12}}{{c}}{{\\\\cellcolor{{Gray!25}}\\\\textbf{{First aggregation level - {} columns}}}} \\\\\\\\ \\\\hline\\n'.format(col_type[test])\n",
    "    \n",
    "    for i, m in enumerate(order):\n",
    "        table += '\\t\\\\texttt{{{}}} & '.format(model_name[m])\n",
    "        tmp_rank = []\n",
    "        for j, s in enumerate(stats_str):\n",
    "            tmp = avg[test][s][m]\n",
    "            tmp_rank.append(tmp['rank'])\n",
    "            all_ranks[m]['stats'].append(tmp['rank'])\n",
    "            \n",
    "            if tmp['rank'] <=10:\n",
    "                suff = '\\\\bf'\n",
    "            else:\n",
    "                suff = ''\n",
    "                \n",
    "            val = round(100*(tmp['rank']-1)/(len(order)-1))\n",
    "                \n",
    "            table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$'.format(val, suff, tmp['rank'], val, suff, tmp['mean'])\n",
    "            \n",
    "            table += ' & '\n",
    "            \n",
    "        # Avg rank\n",
    "        avg_rank = np.mean(tmp_rank)\n",
    "\n",
    "        if avg_rank <=10:\n",
    "            suff = '\\\\bf'\n",
    "        else:\n",
    "            suff = ''\n",
    "\n",
    "        val = round(100*(avg_rank-1)/(len(order)-1))\n",
    "\n",
    "        table += '\\\\cellcolor{{Gray!{}}}${} {:.1f}$  \\\\\\\\'.format(val, suff, avg_rank)\n",
    "            \n",
    "        if (i+1)%12 == 0 and (i+1) < len(order):\n",
    "            table += ' \\\\hline'\n",
    "\n",
    "        table += '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics - second level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pickle.load(open('./{}/couple_combinations.pickle'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 2):\n",
    "    combs.append(k[0] + '::' + k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in order:\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in order:\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_list = [k for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]]\n",
    "    else:\n",
    "        sorted_list = [k for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])]\n",
    "\n",
    "    for i, m in enumerate(sorted_list):\n",
    "        avg[s][m]['rank'] = i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table += '\\t\\\\hline \\\\multicolumn{12}{c}{\\\\cellcolor{Gray!25}\\\\textbf{Second aggregation level}} \\\\\\\\ \\\\hline\\n'\n",
    "\n",
    "\n",
    "for i, m in enumerate(order):\n",
    "    table += '\\t\\\\texttt{{{}}} & '.format(model_name[m])\n",
    "    tmp_rank = []\n",
    "    for j, s in enumerate(stats_str):\n",
    "        tmp = avg[s][m]\n",
    "        tmp_rank.append(tmp['rank'])\n",
    "        all_ranks[m]['stats'].append(tmp['rank'])\n",
    "\n",
    "\n",
    "        if tmp['rank'] <=10:\n",
    "            suff = '\\\\bf'\n",
    "        else:\n",
    "            suff = ''\n",
    "\n",
    "        val = round(100*(tmp['rank']-1)/(len(order)-1))\n",
    "\n",
    "        table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$ & '.format(val, suff, tmp['rank'], val, suff, tmp['mean'])\n",
    "            \n",
    "    # Avg rank\n",
    "    avg_rank = np.mean(tmp_rank)\n",
    "\n",
    "    if avg_rank <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(avg_rank-1)/(len(order)-1))\n",
    "\n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:.1f}$  \\\\\\\\'.format(val, suff, avg_rank)\n",
    "\n",
    "    if (i+1)%12 == 0 and (i+1) < len(order):\n",
    "        table += ' \\\\hline'\n",
    "\n",
    "    table += '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats - third level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pickle.load(open('./{}/trouple_combinations.pickle'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = []\n",
    "\n",
    "for k in combinations(df_orig.columns, 3):\n",
    "    combs.append(k[0] + '::' + k[1] + '::' + k[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in order:\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for i in range(n_models*n_data):\n",
    "            tmp = []\n",
    "\n",
    "            for c in combs:\n",
    "                tmp.append(stats[m][c][s][i])\n",
    "\n",
    "            res[s][m].append(np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in order:\n",
    "            \n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_list = [k for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]]\n",
    "    else:\n",
    "        sorted_list = [k for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])]\n",
    "\n",
    "    for i, m in enumerate(sorted_list):\n",
    "        avg[s][m]['rank'] = i+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "table += '\\t\\\\hline \\\\multicolumn{12}{c}{\\\\cellcolor{Gray!25}\\\\textbf{Third aggregation level}} \\\\\\\\ \\\\hline\\n'\n",
    "\n",
    "\n",
    "for i, m in enumerate(order):\n",
    "    \n",
    "    table += '\\t\\\\texttt{{{}}} & '.format(model_name[m])\n",
    "    tmp_rank = []\n",
    "    for j, s in enumerate(stats_str):\n",
    "        tmp = avg[s][m]\n",
    "        tmp_rank.append(tmp['rank'])\n",
    "        all_ranks[m]['stats'].append(tmp['rank'])\n",
    "\n",
    "\n",
    "        if tmp['rank'] <=10:\n",
    "            suff = '\\\\bf'\n",
    "        else:\n",
    "            suff = ''\n",
    "\n",
    "        val = round(100*(tmp['rank']-1)/(len(order)-1))\n",
    "\n",
    "        table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$ & '.format(val, suff, tmp['rank'], val, suff, tmp['mean'])\n",
    "            \n",
    "    # Avg rank\n",
    "    avg_rank = np.mean(tmp_rank)\n",
    "\n",
    "    if avg_rank <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(avg_rank-1)/(len(order)-1))\n",
    "\n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:.1f}$  \\\\\\\\'.format(val, suff, avg_rank)\n",
    "\n",
    "    if (i+1)%12 == 0 and (i+1) < len(order):\n",
    "        table += ' \\\\hline'\n",
    "\n",
    "    table += '\\n'\n",
    "\n",
    "table += '\\\\end{xltabular}\\n'\n",
    "\n",
    "\n",
    "with open('./tables/stats_final_{}.tex'.format(dataset), 'w') as infile:\n",
    "    infile.write(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML efficacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_modelscores = pickle.load(open('./{}/cv_result_ml.pickle'.format(dataset), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Chicago' in dataset:\n",
    "    cont_cols = ['distance', 'age', 'departure_time']\n",
    "    ord_cols = ['hh_vehicles', 'hh_size', 'hh_bikes', 'hh_income', 'education_level']\n",
    "    cat_cols = [col for col in df_orig.columns if col not in cont_cols + ord_cols]\n",
    "elif 'LPMC' in dataset:\n",
    "    cont_cols = ['start_time_linear', 'age', 'distance', 'dur_walking', \n",
    "                 'dur_cycling', 'dur_pt_access', 'dur_pt_rail', 'dur_pt_bus', \n",
    "                 'dur_pt_int', 'dur_driving', 'cost_transit', \n",
    "                 'cost_driving_fuel', 'driving_traffic_percent']\n",
    "    ord_cols = ['travel_year', 'travel_month', 'travel_date', \n",
    "                'day_of_week', 'pt_n_interchanges', 'car_ownership']\n",
    "    cat_cols = [col for col in df_orig.columns if col not in cont_cols + ord_cols]\n",
    "elif 'adult' in dataset:\n",
    "    cont_cols = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "    ord_cols = []\n",
    "    cat_cols = [col for col in df_orig.columns if col not in cont_cols + ord_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_scores = {col: cv_modelscores['original'][0][col]['test_log_loss'] for col in cat_cols + ord_cols}\n",
    "ori_scores.update({col: cv_modelscores['original'][0][col]['test_l2'] for col in cont_cols})\n",
    "\n",
    "internal = {}\n",
    "external = {}\n",
    "external_normalised = {}\n",
    "cont_scores = {}\n",
    "cat_scores = {}\n",
    "\n",
    "for model in order:\n",
    "    \n",
    "    n_tests = len(cv_modelscores[model])\n",
    "    \n",
    "    internal[model] = {}\n",
    "    external[model] = {}\n",
    "    external_normalised[model] = {}\n",
    "    for col in cat_cols + ord_cols:\n",
    "        tmp = [cv_modelscores[model][i][col]['test_log_loss'] for i in range(n_tests)]\n",
    "        internal[model][col] = {'avg': np.mean(tmp), 'std': np.std(tmp)}\n",
    "        \n",
    "        tmp = [cv_modelscores[model][i][col]['original_log_loss'] for i in range(n_tests)]\n",
    "        external[model][col] = {'avg': np.mean(tmp), 'std': np.std(tmp)}\n",
    "        \n",
    "        external_normalised[model][col] = external[model][col]['avg'] - ori_scores[col]\n",
    "\n",
    "        \n",
    "    for col in cont_cols:\n",
    "        tmp = [cv_modelscores[model][i][col]['test_l2'] for i in range(n_tests)]\n",
    "        internal[model][col] = {'avg': np.mean(tmp), 'std': np.std(tmp)}\n",
    "        \n",
    "        tmp = [cv_modelscores[model][i][col]['original_l2'] for i in range(n_tests)]\n",
    "        external[model][col] = {'avg': np.mean(tmp), 'std': np.std(tmp)}\n",
    "        \n",
    "        external_normalised[model][col] = external[model][col]['avg'] - ori_scores[col]\n",
    "    \n",
    "    cont_scores[model] = sum([external[model][col]['avg']/ori_scores[col] for col in cont_cols])\n",
    "    cat_scores[model] = sum([external[model][col]['avg']-ori_scores[col] for col in cat_cols + ord_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sorted = sorted(cat_scores.items(), key=lambda item: item[1])\n",
    "cont_sorted = sorted(cont_scores.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = '\\\\begin{{xltabular}}{{\\\\textwidth}}{{l|{}}}\\n'.format('|CC'*2+'||C')\n",
    "\n",
    "str_ = ''\n",
    "for i, s in enumerate(['Continuous', 'Categorical']):\n",
    "    str_ += '& \\multicolumn{{2}}{{c{}}}{{\\\\textbf{{{}}}}} '.format((i+1)*'|', s)\n",
    "    \n",
    "str_ += '& \\\\multicolumn{1}{c}{\\\\textbf{rank}} '\n",
    "\n",
    "header = '\\\\multicolumn{{1}}{{c||}}{{\\\\textbf{{Name}}}} {} \\\\\\\\ \\\\midrule[1.5pt]\\n'.format(str_)\n",
    "\n",
    "table += '\\\\caption{{\\\\normalsize Results of the Machine Learning efficacy between the best DATGAN version and the state-of-the-art models for the {} dataset. Lighter grey tone corresponds to better results compared to darker ones.}}\\n'.format(dataset_name[dataset])\n",
    "table += '\\\\label{{tab:ml_efficacy_final_{}}}\\\\\\\\\\n\\n'.format(dataset)\n",
    "table += header\n",
    "table += '\\endfirsthead\\n\\n'\n",
    "table += '\\\\multicolumn{6}{c}{\\\\tablename\\\\ \\\\thetable{} -- continued from previous page} \\\\\\\\\\n'\n",
    "table += header\n",
    "table += '\\endhead\\n\\n'\n",
    "table += '\\\\hline\\\\multicolumn{6}{r}{{\\\\normalsize Continues on next page...}}\\n'\n",
    "table += '\\endfoot\\n\\n'\n",
    "table += '\\endlastfoot\\n\\n'\n",
    "\n",
    "for i, m in enumerate(order):\n",
    "    table += '\\t\\\\texttt{{{}}} & '.format(model_name[m])\n",
    "    # continuous\n",
    "    rank_cont = [x+1 for x, y in enumerate(cont_sorted) if y[0] == m][0]\n",
    "    all_ranks[m]['ml'].append(rank_cont)\n",
    "\n",
    "    value = cont_sorted[rank_cont-1][1]\n",
    "\n",
    "    if rank_cont <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(rank_cont-1)/(len(order)-1))\n",
    "\n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$ & '.format(val, suff, rank_cont, val, suff, value)\n",
    "\n",
    "    # categorical\n",
    "    rank_cat = [x+1 for x, y in enumerate(cat_sorted) if y[0] == m][0]\n",
    "    all_ranks[m]['ml'].append(rank_cat)\n",
    "\n",
    "    value = cat_sorted[rank_cat-1][1]\n",
    "\n",
    "    if rank_cat <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(rank_cat-1)/(len(order)-1))\n",
    "\n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:02}$ & \\\\cellcolor{{Gray!{}}}${} \\\\num{{{:.2e}}}$ &'.format(val, suff, rank_cat, val, suff, value)\n",
    "    \n",
    "    # Avg rank\n",
    "    avg_rank = (rank_cont + rank_cat)/2\n",
    "    \n",
    "    if avg_rank <=10:\n",
    "        suff = '\\\\bf'\n",
    "    else:\n",
    "        suff = ''\n",
    "\n",
    "    val = round(100*(avg_rank-1)/(len(order)-1))\n",
    "    \n",
    "    table += '\\\\cellcolor{{Gray!{}}}${} {:.1f}$  \\\\\\\\'.format(val, suff, avg_rank)\n",
    "    \n",
    "    if (i+1) < len(order) and (i+1)%12 == 0:\n",
    "        table += ' \\\\hline'\n",
    "\n",
    "    table += '\\n'\n",
    "\n",
    "table += '\\\\end{xltabular}\\n'\n",
    "\n",
    "with open('./tables/ml_efficacy_final_{}.tex'.format(dataset), 'w') as infile:\n",
    "    infile.write(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = {}\n",
    "\n",
    "best_stat_rank = 100\n",
    "best_ml_rank = 100\n",
    "\n",
    "for m in order:\n",
    "    ranks[m] = {'stats': np.mean(all_ranks[m]['stats']), 'ml': np.mean(all_ranks[m]['ml'])}\n",
    "    ranks[m]['avg'] = (ranks[m]['stats'] + ranks[m]['ml'])/2\n",
    "    \n",
    "    best_stat_rank = min(best_stat_rank, ranks[m]['stats'])\n",
    "    best_ml_rank = min(best_ml_rank, ranks[m]['ml'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = [k for k, v in sorted(ranks.items(), key=lambda item: item[1]['avg'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = '\\\\begin{{tabularx}}{{0.7\\\\textwidth}}{{l|{}}}\\n'.format('|C'*2+'||C')\n",
    "\n",
    "str_ = ''\n",
    "for i, s in enumerate(['Avg. rank stats', 'Avg. rank ML']):\n",
    "    str_ += '& \\multicolumn{{1}}{{c{}}}{{\\\\textbf{{{}}}}} '.format((i+1)*'|', s)\n",
    "    \n",
    "str_ += '& \\\\multicolumn{1}{c}{\\\\textbf{rank}} '\n",
    "\n",
    "table += '\\\\multicolumn{{1}}{{c||}}{{\\\\textbf{{Name}}}} {} \\\\\\\\ \\\\midrule[1.5pt]\\n'.format(str_)\n",
    "\n",
    "for i in range(len(order)):\n",
    "    m = sorted_list[i]\n",
    "    \n",
    "    table += '\\t\\\\texttt{{{}}} & '.format(model_name[m])\n",
    "    \n",
    "    if ranks[m]['stats'] == best_stat_rank:\n",
    "        table += '\\\\textbf{{{:.2f}}} & '.format(ranks[m]['stats'])\n",
    "    else:\n",
    "        table += '{:.2f} & '.format(ranks[m]['stats'])\n",
    "\n",
    "    if ranks[m]['ml'] == best_ml_rank:\n",
    "        table += '\\\\textbf{{{:.1f}}} & '.format(ranks[m]['ml'])\n",
    "    else:\n",
    "        table += '{:.1f} & '.format(ranks[m]['ml'])\n",
    "    \n",
    "    if i == 0:\n",
    "        table += '\\\\textbf{{{:.2f}}}'.format(ranks[m]['avg'])\n",
    "    else:\n",
    "        table += '{:.2f}'.format(ranks[m]['avg'])\n",
    "        \n",
    "    table += ' \\\\\\\\\\n'\n",
    "\n",
    "table += '\\\\end{tabularx}\\n'\n",
    "\n",
    "with open('./tables/summary_final_{}.tex'.format(dataset), 'w') as infile:\n",
    "    infile.write(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
